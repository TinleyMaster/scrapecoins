#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tokenomist.ai ä»£å¸è§£é”ä¿¡æ¯çˆ¬è™«

ä¾èµ–å®‰è£…:
pip install playwright pymongo
playwright install chromium

æŠ€æœ¯æ–¹æ¡ˆ:
- ä½¿ç”¨ Playwright å¤„ç† JavaScript åŠ¨æ€æ¸²æŸ“
- éµå®ˆ robots.txt è§„åˆ™ (Allow: /*)
- é€šè¿‡ DOM è§£ææå–æ•°æ®ï¼Œé¿å…ç›´æ¥è°ƒç”¨ API
"""

import asyncio
import re
import csv
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional, Callable
from playwright.async_api import async_playwright, Browser, Page
import pymongo
from pymongo import MongoClient
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class TokenUnlockData:
    """ä»£å¸è§£é”æ•°æ®æ¨¡å‹"""
    
    def __init__(self, data: Dict[str, Any] = None):
        if data:
            self.token_name = data.get('token_name', '')
            self.unlock_time = data.get('unlock_time', '')
            self.unlock_amount = data.get('unlock_amount', '')
            self.unlock_percentage = data.get('unlock_percentage', '')
            self.current_price = data.get('current_price', '')
            self.price_change_24h = data.get('price_change_24h', '')
            self.market_cap = data.get('market_cap', '')
            self.circulating_supply = data.get('circulating_supply', '')
            self.released_percentage = data.get('released_percentage', '')
            self.next_7d_emission = data.get('next_7d_emission', '')
            self.source = data.get('source', 'tokenomist.ai')
            self.timestamp = data.get('timestamp', datetime.utcnow())
        else:
            self.token_name = ''
            self.unlock_time = ''
            self.unlock_amount = ''
            self.unlock_percentage = ''
            self.current_price = ''
            self.price_change_24h = ''
            self.market_cap = ''
            self.circulating_supply = ''
            self.released_percentage = ''
            self.next_7d_emission = ''
            self.source = 'tokenomist.ai'
            self.timestamp = datetime.utcnow()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'token_name': self.token_name,
            'unlock_time': self.unlock_time,
            'unlock_amount': self.unlock_amount,
            'unlock_percentage': self.unlock_percentage,
            'current_price': self.current_price,
            'price_change_24h': self.price_change_24h,
            'market_cap': self.market_cap,
            'circulating_supply': self.circulating_supply,
            'released_percentage': self.released_percentage,
            'next_7d_emission': self.next_7d_emission,
            'source': self.source,
            'timestamp': self.timestamp
        }

class TokenomistScraper:
    """Tokenomist.ai çˆ¬è™«ç±»"""
    
    def __init__(self, should_stop: Optional[Callable[[], bool]] = None):
        self.base_url = "https://tokenomist.ai/"
        self.alt_base_urls = ["https://www.tokenomist.ai/"]
        self.user_agent = (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        self.browser_config = {
            "headless": True,
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-gpu",
            ],
        }
        # ä»ç¯å¢ƒå˜é‡æ³¨å…¥ä»£ç†ï¼ˆå¦‚éœ€è¦ï¼‰
        proxy_server = os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
        if proxy_server:
            self.browser_config["proxy"] = {"server": proxy_server}

        # åœæ­¢å›è°ƒï¼ˆé»˜è®¤å§‹ç»ˆè¿”å› Falseï¼‰
        self.should_stop = should_stop or (lambda: False)

        # Mongo ç›¸å…³
        self.client: Optional[MongoClient] = None
        self.db = None
        self.collection = None

        # é‡è¯•é…ç½®
        self.max_retries = 3
        self.retry_delay = 2
        self.mongo_uri = os.getenv('MONGO_URI', 'mongodb://localhost:27017/crypto_db')
        self.db_name = 'crypto_db'
        self.collection_name = 'token_unlocks'
        
        # æµè§ˆå™¨é…ç½®
        self.browser_config = {
            'headless': True,  # æ— å¤´æ¨¡å¼
            'args': [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--disable-gpu',
                '--window-size=1920x1080'
            ]
        }
        
        # ç”¨æˆ·ä»£ç† - æ¨¡æ‹Ÿ Chrome æµè§ˆå™¨
        self.user_agent = (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/119.0.0.0 Safari/537.36'
        )
        
        # é‡è¯•é…ç½®
        self.max_retries = 3
        self.retry_delay = 2
        
        # MongoDB è¿æ¥
        self.client = None
        self.db = None
        self.collection = None
    
    def connect_to_mongodb(self):
        """è¿æ¥åˆ° MongoDB æ•°æ®åº“"""
        try:
            self.client = MongoClient(self.mongo_uri)
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            
            # æµ‹è¯•è¿æ¥
            self.client.admin.command('ping')
            print("âœ“ MongoDB è¿æ¥æˆåŠŸ")
            
            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            self.collection.create_index([
                ('token_name', pymongo.ASCENDING),
                ('timestamp', pymongo.DESCENDING)
            ])
            self.collection.create_index('unlock_time')
            self.collection.create_index('source')
            
        except Exception as e:
            print(f"âœ— MongoDB è¿æ¥å¤±è´¥: {e}")
            raise
    
    def close_mongodb_connection(self):
        """å…³é—­ MongoDB è¿æ¥"""
        if self.client:
            self.client.close()
            print("âœ“ MongoDB è¿æ¥å·²å…³é—­")
    
    async def setup_page(self, browser: Browser) -> Page:
        """è®¾ç½®é¡µé¢é…ç½®"""
        context = await browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080},
            java_script_enabled=True,
            ignore_https_errors=True,  # å¯é€‰
        )

        # æ‹¦æˆªå¹¶å±è”½é‡èµ„æºï¼Œæå‡åŠ è½½é€Ÿåº¦ä¸ç¨³å®šæ€§
        async def _route_handler(route):
            rtype = route.request.resource_type
            if rtype in ("image", "media", "font"):
                await route.abort()
            else:
                await route.continue_()
        await context.route("**/*", _route_handler)

        # åˆ›å»ºæ–°é¡µé¢
        page = await context.new_page()

        # æå‡é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆæ³¨æ„ï¼šè¿™äº›æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¸èƒ½ awaitï¼‰
        page.set_default_navigation_timeout(90000)  # 90s
        page.set_default_timeout(45000)             # 45s

        # è®¾ç½®é¢å¤–çš„è¯·æ±‚å¤´ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«
        await page.set_extra_http_headers({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

        return page
    
    async def wait_for_table_data(self, page: Page) -> bool:
        """ç­‰å¾…è¡¨æ ¼æ•°æ®åŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…è¡¨æ ¼å®¹å™¨å‡ºç°
            await page.wait_for_selector('table, .table, [data-testid="table"]', timeout=30000)
            print("âœ“ è¡¨æ ¼å®¹å™¨å·²åŠ è½½")
            
            # ç­‰å¾…è¡¨æ ¼è¡Œæ•°æ®å‡ºç°ï¼ˆè‡³å°‘æœ‰ä¸€è¡Œæ•°æ®ï¼‰
            await page.wait_for_selector('tbody tr, .table-row', timeout=30000)
            print("âœ“ è¡¨æ ¼æ•°æ®å·²åŠ è½½")
            
            # é¢å¤–ç­‰å¾… JavaScript å®Œå…¨æ¸²æŸ“ï¼ˆé¿å…æ•°æ®ä¸å®Œæ•´ï¼‰
            await page.wait_for_timeout(3000)
            
            return True
            
        except Exception as e:
            print(f"âœ— ç­‰å¾…è¡¨æ ¼æ•°æ®è¶…æ—¶: {e}")
            return False
    
    def parse_unlock_time(self, time_text: str) -> str:
        """è§£æè§£é”æ—¶é—´ä¸ºå…·ä½“ UTC æ—¥æœŸæ—¶é—´ï¼ˆISO æ ¼å¼ï¼‰ã€‚æ”¯æŒç»å¯¹æ—¥æœŸå’Œç›¸å¯¹æ—¶é—´ã€‚"""
        if not time_text:
            return ''
        
        text = re.sub(r'\s+', ' ', time_text.strip())
    
        # 1) ç»å¯¹æ—¥æœŸè§£æï¼šå…ˆä»æ–‡æœ¬ä¸­æå–å‡ºå¯èƒ½çš„æ—¥æœŸç‰‡æ®µï¼Œå†å°è¯•ä¸åŒæ ¼å¼
        absolute_patterns = [
            # çº¯æ—¥æœŸç‰‡æ®µæ•è·
            (r'(\d{4}-\d{2}-\d{2}(?:[ T]\d{2}:\d{2})?)', ['%Y-%m-%d', '%Y-%m-%d %H:%M', '%Y-%m-%dT%H:%M']),
            (r'(\d{1,2}/\d{1,2}/\d{4}(?:\s+\d{2}:\d{2})?)', ['%m/%d/%Y', '%m/%d/%Y %H:%M']),
            (r'(\d{1,2}\s+[A-Za-z]{3,9},?\s*\d{4})', ['%d %b %Y', '%d %B %Y', '%d %b, %Y', '%d %B, %Y']),
            (r'([A-Za-z]{3,9}\s+\d{1,2},\s*\d{4})', ['%b %d, %Y', '%B %d, %Y']),
        ]
        for pattern, fmts in absolute_patterns:
            m = re.search(pattern, text)
            if m:
                piece = m.group(1)
                for fmt in fmts:
                    try:
                        dt = datetime.strptime(piece, fmt)
                        # æ— æ—¶åŒºä¿¡æ¯ï¼ŒæŒ‰ UTC 00:00 å¤„ç†ï¼ˆæˆ–ä¿æŒå·²æœ‰çš„æ—¶åˆ†ï¼‰
                        if dt.tzinfo is None:
                            dt = dt.replace(tzinfo=timezone.utc)
                        # è‹¥æ ¼å¼åªæœ‰æ—¥æœŸï¼Œè¡¥é½ 00:00:00
                        if fmt in ['%Y-%m-%d', '%m/%d/%Y', '%d %b %Y', '%d %B %Y', '%d %b, %Y', '%d %B, %Y']:
                            dt = datetime(dt.year, dt.month, dt.day, 0, 0, 0, tzinfo=timezone.utc)
                        return dt.strftime('%Y-%m-%dT%H:%M:%SZ')
                    except Exception:
                        continue
    
        # 2) ç›¸å¯¹æ—¶é—´è§£æï¼šä¾‹å¦‚ "3d 2h", "in 5 days", "2 hours 30 minutes", "1w 2d"
        # ç´¯åŠ  weeks/days/hours/minutes/seconds
        total = timedelta(0)
        for num, unit in re.findall(r'(\d+)\s*(weeks?|w|days?|d|hours?|h|minutes?|mins?|m|seconds?|secs?|s)', text, flags=re.I):
            n = int(num)
            u = unit.lower()
            if u.startswith('w'):
                total += timedelta(weeks=n)
            elif u.startswith('d'):
                total += timedelta(days=n)
            elif u.startswith('h'):
                total += timedelta(hours=n)
            elif u.startswith('m'):
                total += timedelta(minutes=n)
            elif u.startswith('s'):
                total += timedelta(seconds=n)
    
        if total.total_seconds() > 0 or re.search(r'\bin\s+\d+', text, flags=re.I):
            # "in X ..." åœºæ™¯æ²¡åŒ¹é…åˆ°å•ä½ä¹Ÿç®—ç›¸å¯¹æ—¶é—´çš„æç¤ºï¼Œè¿™é‡Œä»…åœ¨æ•åˆ°å•ä½æ—¶ç”Ÿæ•ˆ
            if total.total_seconds() > 0:
                dt = datetime.now(timezone.utc) + total
                return dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    
        # æœªèƒ½è§£æï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆé¿å…æŠŠåŸå§‹æè¿°ä½œä¸ºæ—¶é—´å†™å…¥ DBï¼‰
        return ''
    
    def clean_amount_text(self, amount_text: str) -> str:
        """æ¸…ç†è§£é”é‡‘é¢æ–‡æœ¬"""
        if not amount_text:
            return ''
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        amount_text = re.sub(r'\s+', ' ', amount_text.strip())
        
        # ä¼˜å…ˆåŒ¹é…ç¾å…ƒé‡‘é¢æ ¼å¼ $X.XXm, $X.XXb, $X.XXk
        dollar_match = re.search(r'\$[\d,]+\.?\d*[kmb]?', amount_text, re.I)
        if dollar_match:
            return dollar_match.group(0)
        
        # åŒ¹é…ä»£å¸æ•°é‡æ ¼å¼ X.XX TOKEN
        token_match = re.search(r'([\d,.]+ \w+)', amount_text)
        if token_match:
            return token_match.group(1)
        
        # åŒ¹é…çº¯æ•°å­—+å•ä½æ ¼å¼ X.XXm, X.XXb, X.XXk
        number_match = re.search(r'[\d,]+\.?\d*[kmb]?', amount_text, re.I)
        if number_match:
            return number_match.group(0)
        
        return amount_text
    
    def clean_price_text(self, price_text: str) -> str:
        """æ¸…ç†ä»·æ ¼æ–‡æœ¬"""
        if not price_text:
            return ''
        
        # æå–ä»·æ ¼æ ¼å¼ $X.XX
        match = re.search(r'\$[\d,]+\.?\d*', price_text)
        if match:
            return match.group(0)
        
        return price_text.strip()
    
    def clean_percentage_text(self, percent_text: str) -> str:
        """æ¸…ç†ç™¾åˆ†æ¯”æ–‡æœ¬"""
        if not percent_text:
            return ''
        
        # æå–ç™¾åˆ†æ¯”æ ¼å¼ +/-X.XX%
        match = re.search(r'[+-]?\d+\.?\d*%', percent_text)
        if match:
            return match.group(0)
        
        return percent_text.strip()
    
    async def extract_table_data(self, page: Page) -> List[Dict[str, Any]]:
        """æå–è¡¨æ ¼æ•°æ®ï¼ˆè·å–æ‰€æœ‰å­—æ®µï¼‰"""
        data_list = []
        
        try:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„è¡¨æ ¼è¡Œé€‰æ‹©å™¨
            rows = await page.query_selector_all('table tbody tr.group.cursor-pointer')
            print(f"âœ“ æ‰¾åˆ° {len(rows)} æ¡æ•°æ®è¡Œ")
            
            for i, row in enumerate(rows):
                try:
                    cells = await row.query_selector_all('td')
                    if len(cells) < 9:  # è·³è¿‡å¹¿å‘Šè¡Œæˆ–ä¸å®Œæ•´è¡Œï¼ˆéœ€è¦è‡³å°‘9åˆ—ï¼‰
                        continue
                    
                    # 1) é¡¹ç›®ç¬¦å·ï¼ˆç¬¬2åˆ— - Project Nameï¼‰
                    name_cell = cells[1]
                    name_text = await name_cell.inner_text()
                    token_name = name_text.strip().split('\n')[0].strip()
                    
                    # 2) ä»·æ ¼ï¼ˆç¬¬3åˆ— - Priceï¼‰
                    price_text = await cells[2].inner_text()
                    current_price = self.clean_price_text(price_text)
                    
                    # 3) 24h æ¶¨å¹…ï¼ˆç¬¬4åˆ— - 24h %ï¼‰
                    pct_text = await cells[3].inner_text()
                    price_change_24h = self.clean_percentage_text(pct_text)
                    
                    # 4) å¸‚å€¼ï¼ˆç¬¬5åˆ— - M.Capï¼‰
                    market_cap_text = await cells[4].inner_text()
                    market_cap = self.clean_amount_text(market_cap_text)
                    
                    # 5) æµé€šä¾›åº”é‡ï¼ˆç¬¬6åˆ— - Cir. Supplyï¼‰
                    cir_supply_text = await cells[5].inner_text()
                    circulating_supply = self.clean_amount_text(cir_supply_text)
                    
                    # 6) é‡Šæ”¾ç™¾åˆ†æ¯”ï¼ˆç¬¬7åˆ— - Released Percentageï¼‰
                    released_pct_cell = cells[6]
                    released_pct_text = await released_pct_cell.inner_text()
                    released_percentage = self.clean_percentage_text(released_pct_text)
                    
                    # 7) å³å°†è§£é”ï¼ˆç¬¬8åˆ— - Upcoming Unlockï¼‰
                    upcoming_text = await cells[7].inner_text()
                    upcoming_clean = re.sub(r'\s+', ' ', upcoming_text.strip())
                    
                    # å…ˆæå–é‡‘é¢ï¼ˆä¼˜å…ˆåŒ¹é…ç¾å…ƒæ ¼å¼ï¼‰
                    unlock_amount = self.clean_amount_text(upcoming_clean)
                    
                    # æå–ç™¾åˆ†æ¯”
                    unlock_percentage = ''
                    pct_match = re.search(r'(\d+\.\d+%)', upcoming_clean)
                    if pct_match:
                        unlock_percentage = pct_match.group(1)
                    
                    # æå–å€’è®¡æ—¶ä¿¡æ¯ï¼ˆåŒ¹é… XD XH XM XS æ ¼å¼ï¼‰
                    countdown_match = re.search(r'(\d+D(?:\s*\d+H)?(?:\s*\d+M)?(?:\s*\d+S)?)', upcoming_clean, re.I)
                    if not countdown_match:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å€’è®¡æ—¶ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                        countdown_match = re.search(r'(\d+d(?:\s*\d+h)?|in\s+\d+\s+days?|\d+\s+days?|\d+\s+hours?)', upcoming_clean, re.I)
                    
                    countdown_text = countdown_match.group(1) if countdown_match else ''
                    
                    # è§£æä¸ºå…·ä½“ UTC æ—¶é—´
                    unlock_time = self.parse_unlock_time(countdown_text or upcoming_clean)
                    
                    # 8) æœªæ¥7å¤©æ’æ”¾é‡ï¼ˆç¬¬9åˆ— - Next 7D Emissionï¼‰
                    if len(cells) > 8:
                        next_7d_text = await cells[8].inner_text()
                        next_7d_emission = self.clean_amount_text(next_7d_text)
                    else:
                        next_7d_emission = ''
                    
                    if i < 3:
                        print(f"ğŸ” è¡Œ {i+1}: {token_name} | ä»·æ ¼: {current_price} | å¸‚å€¼: {market_cap} | æµé€šé‡: {circulating_supply} | é‡Šæ”¾%: {released_percentage} | è§£é”: {unlock_amount} ({unlock_percentage}) | å€’è®¡æ—¶: {countdown_text} | æ—¶é—´: {unlock_time} | 7å¤©æ’æ”¾: {next_7d_emission}")
                    
                    # æ„å»ºå®Œæ•´æ•°æ®
                    token_data = {
                        'token_name': token_name,
                        'current_price': current_price,
                        'price_change_24h': price_change_24h,
                        'market_cap': market_cap,
                        'circulating_supply': circulating_supply,
                        'released_percentage': released_percentage,
                        'unlock_amount': unlock_amount,
                        'unlock_percentage': unlock_percentage,
                        'unlock_time': unlock_time,
                        'next_7d_emission': next_7d_emission,
                        'source': 'tokenomist.ai',
                        'timestamp': datetime.utcnow()
                    }
                    
                    # åªè¦æœ‰é¡¹ç›®åå°±è®°å½•
                    if token_name:
                        data_list.append(token_data)
                        
                except Exception as e:
                    print(f"âŒ è§£æç¬¬ {i+1} è¡Œæ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"âœ… æˆåŠŸæå– {len(data_list)} æ¡æœ‰æ•ˆæ•°æ®")
            return data_list
            
        except Exception as e:
            print(f"âŒ æå–è¡¨æ ¼æ•°æ®æ—¶å‡ºé”™: {e}")
            return []
    
    async def scrape_with_retry(self) -> List[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„çˆ¬å–æ–¹æ³•"""
        for attempt in range(self.max_retries):
            # å¼€å§‹å°è¯•å‰æ£€æŸ¥åœæ­¢ä¿¡å·
            if self.should_stop():
                print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆå½“å‰çˆ¬å–ä»»åŠ¡")
                return []
            try:
                print(f"ğŸ“¡ å¼€å§‹ç¬¬ {attempt + 1} æ¬¡çˆ¬å–å°è¯•...")
                
                async with async_playwright() as p:
                    # å¯åŠ¨æµè§ˆå™¨
                    browser = await p.chromium.launch(**self.browser_config)

                    try:
                        # è®¾ç½®é¡µé¢
                        page = await self.setup_page(browser)

                        # ä¾æ¬¡å°è¯•ä¸»åŸŸåä¸å¤‡ç”¨åŸŸå
                        last_exc = None
                        for nav_url in [self.base_url] + getattr(self, "alt_base_urls", []):
                            if self.should_stop():
                                print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢å¯¼èˆªå¹¶é€€å‡º")
                                await browser.close()
                                return []
                            try:
                                print(f"ğŸŒ æ­£åœ¨è®¿é—®: {nav_url}")
                                response = await page.goto(
                                    nav_url,
                                    wait_until='domcontentloaded',
                                    timeout=90000
                                )
                                if response is None or (getattr(response, "status", None) is None or response.status < 400):
                                    print(f"âœ“ é¡µé¢åŠ è½½æˆåŠŸ: {nav_url}")
                                    break
                                raise Exception(f"é¡µé¢å“åº”çŠ¶æ€å¼‚å¸¸: {response.status}")
                            except Exception as ne:
                                print(f"âš ï¸ è®¿é—® {nav_url} å¤±è´¥: {ne}")
                                last_exc = ne
                                continue
                        else:
                            # æ‰€æœ‰å€™é€‰ URL éƒ½å¤±è´¥
                            raise last_exc or Exception("æ‰€æœ‰å€™é€‰ URL è®¿é—®å¤±è´¥")

                        # å¯é€‰ï¼šç­‰å¾…åˆ°è¾ƒç©ºé—²çš„ç½‘ç»œçŠ¶æ€ï¼ˆä¸å¼ºåˆ¶ï¼‰
                        try:
                            await page.wait_for_load_state('networkidle', timeout=15000)
                        except Exception:
                            print("âš ï¸ networkidle æœªè¾¾æˆï¼Œç»§ç»­è¿›è¡Œ...")

                        if self.should_stop():
                            print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œè·³è¿‡è¡¨æ ¼ç­‰å¾…å¹¶é€€å‡º")
                            await browser.close()
                            return []

                        # ç­‰å¾…è¡¨æ ¼æ•°æ®åŠ è½½
                        if not await self.wait_for_table_data(page):
                            print("âš ï¸ è¡¨æ ¼æ•°æ®ç¬¬ä¸€æ¬¡ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•åˆ·æ–°é¡µé¢åé‡è¯•ç­‰å¾…...")
                            try:
                                await page.reload(wait_until='domcontentloaded', timeout=60000)
                                await page.wait_for_timeout(2000)
                            except Exception as re:
                                print(f"âš ï¸ åˆ·æ–°é¡µé¢å¼‚å¸¸: {re}")

                            if self.should_stop():
                                print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œåˆ·æ–°åä»åœæ­¢é€€å‡º")
                                await browser.close()
                                return []

                            if not await self.wait_for_table_data(page):
                                raise Exception("è¡¨æ ¼æ•°æ®åŠ è½½è¶…æ—¶")

                        if self.should_stop():
                            print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆæ•°æ®æå–å¹¶é€€å‡º")
                            await browser.close()
                            return []

                        # æå–è¡¨æ ¼æ•°æ®
                        data = await self.extract_table_data(page)
                        if data:
                            print(f"âœ“ çˆ¬å–æˆåŠŸï¼Œè·å– {len(data)} æ¡æ•°æ®")
                            return data
                        else:
                            raise Exception("æœªæå–åˆ°æœ‰æ•ˆæ•°æ®")

                    finally:
                        # å…³é—­æµè§ˆå™¨
                        await browser.close()

            except Exception as e:
                print(f"âœ— ç¬¬ {attempt + 1} æ¬¡çˆ¬å–å¤±è´¥: {e}")

                if attempt < self.max_retries - 1:
                    # é‡è¯•å‰æ£€æŸ¥åœæ­¢ä¿¡å·
                    if self.should_stop():
                        print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢åç»­é‡è¯•")
                        return []
                    print(f"â³ {self.retry_delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(self.retry_delay)
                    self.retry_delay *= 2  # æŒ‡æ•°é€€é¿

        print("âœ— æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
        return []
    
    def save_to_mongodb(self, data_list: List[Dict[str, Any]]) -> bool:
        """ä¿å­˜æ•°æ®åˆ° MongoDBï¼ˆå»é‡ï¼šæŒ‰ source + token_name upsertï¼‰"""
        if not data_list:
            print("âš ï¸ æ— æ•°æ®éœ€è¦ä¿å­˜")
            return False
        
        try:
            # è½¬æ¢ä¸º TokenUnlockData å¯¹è±¡
            token_unlock_docs = []
            for data in data_list:
                token_unlock = TokenUnlockData(data)
                token_unlock_docs.append(token_unlock.to_dict())
            
            # æ‰¹é‡ upsertï¼Œä½¿ç”¨ç®€åŒ–çš„å»é‡ç­–ç•¥ï¼šä»… source + token_name
            ops = []
            for doc in token_unlock_docs:
                # å»é‡ç­–ç•¥ï¼šåªä½¿ç”¨ source å’Œ token_name
                filter_doc = {
                    'source': doc.get('source', 'tokenomist.ai'),
                    'token_name': doc.get('token_name', '').strip(),  # å»é™¤ç©ºæ ¼
                }
                
                update_doc = {
                    '$set': {
                        # å³å°†è§£é”ç›¸å…³å­—æ®µ
                        'unlock_time': doc.get('unlock_time', ''),
                        'unlock_amount': doc.get('unlock_amount', ''),
                        'unlock_percentage': doc.get('unlock_percentage', ''),
                        # å…¶ä»–å­—æ®µ
                        'current_price': doc.get('current_price', ''),
                        'price_change_24h': doc.get('price_change_24h', ''),
                        'market_cap': doc.get('market_cap', ''),
                        'circulating_supply': doc.get('circulating_supply', ''),
                        'released_percentage': doc.get('released_percentage', ''),
                        'next_7d_emission': doc.get('next_7d_emission', ''),
                        'timestamp': doc.get('timestamp', datetime.utcnow()),
                    }
                }
                ops.append(pymongo.UpdateOne(filter_doc, update_doc, upsert=True))
            
            if not ops:
                print("âš ï¸ æ— å¯å†™å…¥æ“ä½œ")
                return False
            
            result = self.collection.bulk_write(ops, ordered=False)
            upserted = result.upserted_count
            modified = result.modified_count
            matched = result.matched_count
            print(f"âœ“ å»é‡å†™å…¥å®Œæˆï¼šupsert={upserted}, modified={modified}, matched={matched}")
            return True
        
        except Exception as e:
            print(f"âœ— ä¿å­˜åˆ° MongoDB å¤±è´¥: {e}")
            return False
    
    def save_to_csv(self, data_list: List[Dict[str, Any]], archive_dir: str = 'data/archives') -> bool:
        """ä¿å­˜æ•°æ®åˆ° CSV æ–‡ä»¶ï¼ˆæŒ‰æ—¶é—´å‘½åå­˜æ¡£ï¼‰"""
        if not data_list:
            print("âš ï¸ æ— æ•°æ®éœ€è¦ä¿å­˜åˆ°CSV")
            return False
        
        try:
            # åˆ›å»ºå­˜æ¡£ç›®å½•
            os.makedirs(archive_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'tokenomist_unlocks_{timestamp}.csv'
            filepath = os.path.join(archive_dir, filename)
            
            # ä¿å­˜åˆ°æ—¶é—´æˆ³æ–‡ä»¶
            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                if data_list:
                    fieldnames = data_list[0].keys()
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    
                    # å†™å…¥è¡¨å¤´
                    writer.writeheader()
                    
                    # å†™å…¥æ•°æ®
                    writer.writerows(data_list)
            
            print(f"âœ“ æ•°æ®å·²å­˜æ¡£åˆ°: {filepath}")
            
            # åŒæ—¶ä¿å­˜åˆ°æ ¹ç›®å½•çš„æœ€æ–°æ–‡ä»¶ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            latest_filename = 'token_unlocks.csv'
            with open(latest_filename, 'w', newline='', encoding='utf-8') as csvfile:
                if data_list:
                    fieldnames = data_list[0].keys()
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data_list)
            
            print(f"âœ“ æœ€æ–°æ•°æ®å·²ä¿å­˜åˆ°: {latest_filename}")
            return True
        
        except Exception as e:
            print(f"âœ— ä¿å­˜åˆ° CSV å¤±è´¥: {e}")
            return False

    def save_to_csv_with_cleanup(self, data_list: List[Dict[str, Any]], archive_dir: str = 'data/archives', keep_days: int = 30) -> bool:
        """ä¿å­˜æ•°æ®åˆ° CSV æ–‡ä»¶å¹¶æ¸…ç†æ—§æ–‡ä»¶"""
        if not self.save_to_csv(data_list, archive_dir):
            return False
        
        try:
            # æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ–‡ä»¶
            import time
            current_time = time.time()
            cutoff_time = current_time - (keep_days * 24 * 60 * 60)
            
            if os.path.exists(archive_dir):
                for filename in os.listdir(archive_dir):
                    if filename.startswith('tokenomist_unlocks_') and filename.endswith('.csv'):
                        filepath = os.path.join(archive_dir, filename)
                        file_time = os.path.getctime(filepath)
                        
                        if file_time < cutoff_time:
                            os.remove(filepath)
                            print(f"ğŸ—‘ï¸ å·²æ¸…ç†æ—§æ–‡ä»¶: {filename}")
            
            return True
        
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return True  # ä¿å­˜æˆåŠŸï¼Œæ¸…ç†å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½
        
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return True  # ä¿å­˜æˆåŠŸï¼Œæ¸…ç†å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½

    async def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        print("ğŸš€ Tokenomist.ai ä»£å¸è§£é”ä¿¡æ¯çˆ¬è™«å¯åŠ¨")
        print("=" * 50)
        
        # è¿æ¥æ•°æ®åº“
        self.connect_to_mongodb()
        
        try:
            # æ‰§è¡Œçˆ¬å–
            data = await self.scrape_with_retry()
            
            if data:
                # ä¿å­˜åˆ° MongoDB
                self.save_to_mongodb(data)
                
                # ä¿å­˜åˆ° CSV å­˜æ¡£ï¼ˆå¸¦è‡ªåŠ¨æ¸…ç†ï¼‰
                self.save_to_csv_with_cleanup(data, archive_dir='data/archives', keep_days=30)
                
                print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼è·å– {len(data)} æ¡ä»£å¸è§£é”ä¿¡æ¯")
            else:
                print("ğŸ˜ çˆ¬å–å¤±è´¥ï¼Œæœªè·å–åˆ°æ•°æ®")
        
        finally:
            # å…³é—­æ•°æ®åº“è¿æ¥
            self.close_mongodb_connection()

# ç‹¬ç«‹è¿è¡Œè„šæœ¬
async def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œä»£å¸è§£é”ä¿¡æ¯çˆ¬è™«
    
    åˆè§„è¯´æ˜:
    - éµå®ˆ robots.txt è§„åˆ™ (Allow: /*)
    - ä½¿ç”¨åˆç†çš„è¯·æ±‚é—´éš”å’Œé‡è¯•æœºåˆ¶
    - ä»…çˆ¬å–å…¬å¼€é¡µé¢æ•°æ®ï¼Œä¸è°ƒç”¨ç§æœ‰API
    - ç”Ÿäº§ç¯å¢ƒå»ºè®®å¢åŠ æ›´é•¿çš„è¯·æ±‚é—´éš”
    """
    scraper = TokenomistScraper()
    await scraper.run()

if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    asyncio.run(main())


async def check_pagination_info(self, page: Page) -> Dict[str, Any]:
    """æ£€æŸ¥åˆ†é¡µä¿¡æ¯"""
    try:
        # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯æ–‡æœ¬ "1â€“50 of 28" æ ¼å¼
        pagination_selector = 'div:has-text("of")'
        pagination_elements = await page.query_selector_all(pagination_selector)
        
        for element in pagination_elements:
            text = await element.inner_text()
            # åŒ¹é… "Xâ€“Y of Z" æ ¼å¼
            match = re.search(r'(\d+)\s*â€“\s*(\d+)\s+of\s+(\d+)', text)
            if match:
                start_item = int(match.group(1))
                end_item = int(match.group(2))
                total_items = int(match.group(3))
                items_per_page = end_item - start_item + 1
                total_pages = (total_items + items_per_page - 1) // items_per_page
                current_page = (start_item + items_per_page - 1) // items_per_page
                
                print(f"ğŸ“„ åˆ†é¡µä¿¡æ¯: ç¬¬ {current_page}/{total_pages} é¡µ, æ˜¾ç¤º {start_item}-{end_item}/{total_items} é¡¹")
                
                return {
                    'current_page': current_page,
                    'total_pages': total_pages,
                    'total_items': total_items,
                    'items_per_page': items_per_page,
                    'start_item': start_item,
                    'end_item': end_item
                }
        
        # å¦‚æœæ²¡æ‰¾åˆ°åˆ†é¡µä¿¡æ¯ï¼Œå‡è®¾åªæœ‰ä¸€é¡µ
        print("ğŸ“„ æœªæ‰¾åˆ°åˆ†é¡µä¿¡æ¯ï¼Œå‡è®¾åªæœ‰ä¸€é¡µæ•°æ®")
        return {
            'current_page': 1,
            'total_pages': 1,
            'total_items': 0,
            'items_per_page': 50,
            'start_item': 1,
            'end_item': 50
        }
        
    except Exception as e:
        print(f"âœ— æ£€æŸ¥åˆ†é¡µä¿¡æ¯å¤±è´¥: {e}")
        return {
            'current_page': 1,
            'total_pages': 1,
            'total_items': 0,
            'items_per_page': 50,
            'start_item': 1,
            'end_item': 50
        }

async def navigate_to_next_page(self, page: Page) -> bool:
    """å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ"""
    try:
        # æŸ¥æ‰¾NextæŒ‰é’®
        next_selectors = [
            'button:has-text("Next"):not([disabled])',
            'button[aria-label*="next"]:not([disabled])',
            'button[aria-label*="Next"]:not([disabled])',
            '.pagination button:last-child:not([disabled])'
        ]
        
        next_button = None
        for selector in next_selectors:
            try:
                next_button = await page.query_selector(selector)
                if next_button:
                    print(f"âœ“ æ‰¾åˆ°NextæŒ‰é’®: {selector}")
                    break
            except:
                continue
        
        if not next_button:
            print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„NextæŒ‰é’®ï¼ˆå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µï¼‰")
            return False
        
        # ç‚¹å‡»NextæŒ‰é’®
        print("ğŸ”„ ç‚¹å‡»NextæŒ‰é’®...")
        await next_button.click()
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_timeout(2000)
        
        # ç­‰å¾…è¡¨æ ¼é‡æ–°åŠ è½½
        try:
            await page.wait_for_selector('table tbody tr', timeout=10000)
            print("âœ“ ä¸‹ä¸€é¡µåŠ è½½å®Œæˆ")
            return True
        except:
            print("âš ï¸ ä¸‹ä¸€é¡µè¡¨æ ¼åŠ è½½è¶…æ—¶")
            return False
            
    except Exception as e:
        print(f"âœ— å¯¼èˆªåˆ°ä¸‹ä¸€é¡µå¤±è´¥: {e}")
        return False

async def scrape_all_pages(self, page: Page) -> List[Dict[str, Any]]:
    """çˆ¬å–æ‰€æœ‰åˆ†é¡µæ•°æ®"""
    all_data = []
    current_page = 1
    max_pages = 10  # è®¾ç½®æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé¿å…æ— é™å¾ªç¯
    
    while current_page <= max_pages:
        # æ£€æŸ¥åœæ­¢ä¿¡å·
        if self.should_stop():
            print("â¹ï¸ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢åˆ†é¡µçˆ¬å–")
            break
            
        print(f"ğŸ“– æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µ...")
        
        # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
        pagination_info = await self.check_pagination_info(page)
        
        # ç­‰å¾…å½“å‰é¡µè¡¨æ ¼æ•°æ®åŠ è½½
        if not await self.wait_for_table_data(page):
            print(f"âš ï¸ ç¬¬ {current_page} é¡µè¡¨æ ¼æ•°æ®åŠ è½½å¤±è´¥")
            break
        
        # æå–å½“å‰é¡µæ•°æ®
        page_data = await self.extract_table_data(page)
        if page_data:
            all_data.extend(page_data)
            print(f"âœ“ ç¬¬ {current_page} é¡µæå–åˆ° {len(page_data)} æ¡æ•°æ®")
        else:
            print(f"âš ï¸ ç¬¬ {current_page} é¡µæœªæå–åˆ°æ•°æ®")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
        if current_page >= pagination_info['total_pages']:
            print(f"âœ“ å·²åˆ°è¾¾æœ€åä¸€é¡µ ({pagination_info['total_pages']})")
            break
        
        # å¯¼èˆªåˆ°ä¸‹ä¸€é¡µ
        if not await self.navigate_to_next_page(page):
            print("âš ï¸ æ— æ³•å¯¼èˆªåˆ°ä¸‹ä¸€é¡µï¼Œç»“æŸçˆ¬å–")
            break
        
        current_page += 1
        
        # é¡µé¢é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await page.wait_for_timeout(1000)
    
    print(f"ğŸ¯ åˆ†é¡µçˆ¬å–å®Œæˆï¼Œæ€»å…±è·å– {len(all_data)} æ¡æ•°æ®")
    return all_data