#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DropsTab æŠ•èµ„è€…æ•°æ®çˆ¬è™«
"""

import requests
import time
import random
import pandas as pd
import re
from typing import List, Dict, Any
from .base_scraper import BaseScraper
from datetime import datetime
from ..database.db import InvestorDataManager
from ..models.investor import InvestorData

class DropstabScraper(BaseScraper):
    """DropsTab æŠ•èµ„è€…æ•°æ®çˆ¬è™«ç±»"""
    
    def __init__(self, page_delay_min=10.0, page_delay_max=20.0, debug=True):
        super().__init__('dropstab', 'https://dropstab.com')
        self.page_delay_min = page_delay_min
        self.page_delay_max = page_delay_max
        self.base_rate_limit_delay = 5.0
        self.debug = debug
        self.max_retries = 3
        self.seen_investors = set()
        
        # è®¾ç½®è¯·æ±‚å¤´
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        self.investor_manager = InvestorDataManager()
        
        if self.debug:
            print("ğŸ”§ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
            print(f"ğŸ“‹ è¯·æ±‚å¤´é…ç½®: {dict(self.session.headers)}")
            print(f"â±ï¸  é¡µé¢å»¶è¿ŸèŒƒå›´: {self.page_delay_min}-{self.page_delay_max}ç§’")

    def get_supported_cryptos(self) -> List[str]:
        """è·å–æ”¯æŒçš„åŠ å¯†è´§å¸åˆ—è¡¨ï¼ˆæŠ•èµ„è€…çˆ¬è™«ä¸éœ€è¦åŠ å¯†è´§å¸åˆ—è¡¨ï¼‰"""
        return []

    def scrape_crypto_data(self, crypto_ids: List[str]) -> List[Dict[str, Any]]:
        """çˆ¬å–åŠ å¯†è´§å¸æ•°æ®ï¼ˆæŠ•èµ„è€…çˆ¬è™«ä¸éœ€è¦å®ç°æ­¤æ–¹æ³•ï¼‰"""
        return []

    def scrape_investors_data(self, max_pages=370):
        """é€šè¿‡APIçˆ¬å–æŠ•èµ„è€…æ•°æ®"""
        all_investors = []
        api_url = "https://api2.icodrops.com/portfolio/api/investors"
        
        print(f"ğŸš€ å¼€å§‹é€šè¿‡APIçˆ¬å–æŠ•èµ„è€…æ•°æ® (æœ€å¤š {max_pages} é¡µ)")
        print(f"ğŸŒ APIæ¥å£: {api_url}")
        print(f"ğŸ’¾ æ¯é¡µæ•°æ®å°†ç«‹å³ä¿å­˜åˆ°MongoDBæ•°æ®åº“")
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        if not self.investor_manager.test_connection():
            print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ä¿å­˜æ•°æ®")
            return []
        
        print("âœ… æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸï¼Œå¼€å§‹çˆ¬å–...")
        
        try:
            for page in range(0, max_pages):  # APIé¡µç ä»0å¼€å§‹
                display_page = page + 1  # ç”¨äºæ˜¾ç¤ºçš„é¡µç ï¼ˆä»1å¼€å§‹ï¼‰
                print(f"\nğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {display_page}/{max_pages} é¡µæŠ•èµ„è€…æ•°æ®...")
                
                # é‡è¯•æœºåˆ¶
                success = False
                for retry in range(self.max_retries):
                    try:
                        # æ„å»ºè¯·æ±‚å‚æ•°
                        params = {
                            "sort": "rank",
                            "order": "ASC",
                            "page": page,
                            "size": 20,
                            "filters": {}  
                        }
                        
                        print(f"ğŸ“¡ APIè¯·æ±‚å‚æ•°: {params} (å°è¯• {retry + 1}/{self.max_retries})")
                        
                        if self.debug:
                            print(f"â±ï¸  æ‰§è¡Œé€Ÿç‡é™åˆ¶å»¶è¿Ÿ...")
                        
                        # å‘é€POSTè¯·æ±‚åˆ°API
                        self._rate_limit()
                        
                        # æ·»åŠ éšæœºå»¶è¿Ÿ
                        extra_delay = random.uniform(1.0, 3.0)
                        time.sleep(extra_delay)
                        
                        start_time = time.time()
                        response = self.session.post(
                            api_url,
                            json=params,  # å‘é€JSONæ•°æ®
                            timeout=60,
                            headers={
                                'Content-Type': 'application/json',
                                'Accept': 'application/json',
                                **self.session.headers
                            }
                        )
                        request_time = time.time() - start_time
                        
                        print(f"ğŸ“Š è¯·æ±‚è€—æ—¶: {request_time:.2f}ç§’, çŠ¶æ€ç : {response.status_code}")
                        if self.debug:
                            print(f"ğŸ“ å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
                        
                        response.raise_for_status()
                        success = True
                        break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                        
                    except requests.exceptions.Timeout:
                        print(f"â° ç¬¬ {display_page} é¡µè¯·æ±‚è¶…æ—¶ (å°è¯• {retry + 1}/{self.max_retries})")
                        if retry < self.max_retries - 1:
                            time.sleep((retry + 1) * 5)
                    except requests.exceptions.ConnectionError as e:
                        print(f"ğŸŒ ç¬¬ {display_page} é¡µè¿æ¥é”™è¯¯ (å°è¯• {retry + 1}/{self.max_retries}): {e}")
                        if retry < self.max_retries - 1:
                            time.sleep((retry + 1) * 10)
                    except requests.exceptions.RequestException as e:
                        print(f"âŒ ç¬¬ {display_page} é¡µè¯·æ±‚å¤±è´¥ (å°è¯• {retry + 1}/{self.max_retries}): {e}")
                        if retry < self.max_retries - 1:
                            time.sleep((retry + 1) * 10)
                
                if not success:
                    print(f"âŒ ç¬¬ {display_page} é¡µæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ")
                    continue
                
                # è§£æJSONå“åº”
                try:
                    api_data = response.json()
                    if self.debug:
                        print(f"ğŸ” APIå“åº”ç»“æ„: {list(api_data.keys()) if isinstance(api_data, dict) else 'N/A'}")
                        
                    # ä»APIå“åº”ä¸­æå–æŠ•èµ„è€…æ•°æ®
                    investors = self._parse_api_response(api_data)
                    
                    if not investors:
                        print(f"âš ï¸  ç¬¬ {display_page} é¡µæ²¡æœ‰æ‰¾åˆ°æŠ•èµ„è€…æ•°æ®")
                        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æœ€åä¸€é¡µ
                        if api_data.get('last', False) or api_data.get('empty', True):
                            print(f"ğŸ“„ å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                            break
                        continue
                    
                    # ç«‹å³ä¿å­˜å½“å‰é¡µé¢æ•°æ®åˆ°æ•°æ®åº“
                    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¬¬ {page + 1} é¡µçš„ {len(investors)} ä¸ªæŠ•èµ„è€…åˆ°æ•°æ®åº“...")
                    saved_count = self._save_page_to_database(investors, page + 1)
                    
                    if saved_count > 0:
                        all_investors.extend(investors)
                        print(f"âœ… ç¬¬ {page + 1} é¡µä¿å­˜æˆåŠŸ: {saved_count}/{len(investors)} ä¸ªæŠ•èµ„è€…å·²ä¿å­˜åˆ°æ•°æ®åº“")
                        print(f"ğŸ“Š ç´¯è®¡å·²ä¿å­˜: {len(all_investors)} ä¸ªæŠ•èµ„è€…")
                    else:
                        print(f"âŒ ç¬¬ {page + 1} é¡µæ•°æ®ä¿å­˜å¤±è´¥")
                    
                    if self.debug and investors:
                        print(f"ğŸ“‹ æœ¬é¡µæŠ•èµ„è€…æ ·æœ¬:")
                        for i, inv in enumerate(investors[:3], 1):
                            print(f"   {i}. {inv.get('name', 'N/A')} - {inv.get('ventureType', 'N/A')} - æ’å: {inv.get('rank', 'N/A')}")
                        if len(investors) > 3:
                            print(f"   ... è¿˜æœ‰ {len(investors) - 3} ä¸ªæŠ•èµ„è€…")
                    
                    # æ£€æŸ¥åˆ†é¡µä¿¡æ¯
                    total_pages = api_data.get('totalPages', 0)
                    current_page = api_data.get('number', 0)
                    is_last = api_data.get('last', False)
                    
                    print(f"ğŸ“„ åˆ†é¡µä¿¡æ¯: ç¬¬ {current_page + 1}/{total_pages} é¡µ")
                    
                    if is_last or current_page >= total_pages - 1:
                        print(f"ğŸ“„ å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–")
                        break
                    
                    # é¡µé¢é—´å»¶è¿Ÿ
                    if page < max_pages - 1:
                        delay = random.uniform(self.page_delay_min, self.page_delay_max)
                        print(f"â±ï¸  ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                        time.sleep(delay)
                        
                except ValueError as e:
                    print(f"âŒ ç¬¬ {page + 1} é¡µJSONè§£æå¤±è´¥: {e}")
                    if self.debug:
                        print(f"ğŸ” å“åº”å†…å®¹é¢„è§ˆ: {response.text[:500]}...")
                    continue
                    
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–ï¼Œå·²è·å–å¹¶ä¿å­˜ {len(all_investors)} ä¸ªæŠ•èµ„è€…æ•°æ®")
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            if self.debug:
                import traceback
                print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(all_investors)} ä¸ªæŠ•èµ„è€…")
        print(f"ğŸ’¾ å·²å…¨éƒ¨ä¿å­˜åˆ°MongoDBæ•°æ®åº“")
        
        return all_investors

    def _parse_api_response(self, api_data):
        """è§£æAPIå“åº”æ•°æ®"""
        investors = []
        
        try:
            # è·å–contentæ•°ç»„
            content = api_data.get('content', [])
            if not content:
                print("âŒ APIå“åº”ä¸­æ²¡æœ‰contentæ•°ç»„")
                return investors
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(content)} ä¸ªæŠ•èµ„è€…è®°å½•")
            
            for i, item in enumerate(content):
                try:
                    # å¤„ç†æ—¶é—´æˆ³è½¬æ¢
                    last_round_date = item.get('lastRoundDate')
                    if last_round_date:
                        # è½¬æ¢æ¯«ç§’æ—¶é—´æˆ³ä¸ºdatetime
                        last_round_date = datetime.fromtimestamp(last_round_date / 1000)
                    
                    # æ˜ å°„APIå­—æ®µåˆ°æœ¬åœ°å­—æ®µ (æ’é™¤coInvestments)
                    investor_data = {
                        # åŸºç¡€ä¿¡æ¯
                        'id': item.get('id'),
                        'name': item.get('name', ''),
                        'investorSlug': item.get('investorSlug', ''),
                        'logo': item.get('logo', ''),
                        'image': item.get('image', ''),
                        'country': item.get('country', {}),
                        'ventureType': item.get('ventureType', ''),
                        'rank': item.get('rank'),
                        'rating': item.get('rating'),
                        'tier': item.get('tier', ''),
                        'lead': item.get('lead', False),
                        'description': item.get('description', ''),
                        
                        # ç¤¾äº¤åª’ä½“å’Œé“¾æ¥
                        'twitterUrl': item.get('twitterUrl', ''),
                        'links': item.get('links', []),
                        'twitterScore': item.get('twitterScore'),
                        
                        # æŠ•èµ„ç»Ÿè®¡
                        'totalInvestments': item.get('totalInvestments'),
                        'leadInvestments': item.get('leadInvestments'),
                        'roundsPerYear': item.get('roundsPerYear'),
                        'publicSalesCount': item.get('publicSalesCount'),
                        'lastRoundDate': last_round_date,
                        
                        # ROIæ•°æ®
                        'avgPublicRoi': item.get('avgPublicRoi', {}),
                        'avgPrivateRoi': item.get('avgPrivateRoi', {}),
                        
                        # å¸å®‰ä¸Šå¸‚æ•°æ®
                        'binanceListed': item.get('binanceListed', {}),
                        
                        # æŠ•èµ„åˆ†å¸ƒ
                        'roundsDistribution': item.get('roundsDistribution', {}),
                        
                        # æŠ•èµ„ç»„åˆé¡¹ç›®
                        'portfolioProjects': item.get('portfolioProjects', []),
                        
                        # é”€å”®ID
                        'saleIds': item.get('saleIds', []),
                        
                        # å…ƒæ•°æ®
                        'source': 'icodrops_api',
                        'scraped_at': datetime.utcnow(),
                        'timestamp': datetime.utcnow()
                    }
                    
                    # éªŒè¯å¿…è¦å­—æ®µ
                    if not investor_data['name']:
                        print(f"âš ï¸ ç¬¬ {i+1} ä¸ªè®°å½•ç¼ºå°‘åç§°ï¼Œè·³è¿‡")
                        continue
                    
                    investors.append(investor_data)
                    
                    if (i + 1) % 10 == 0:
                        print(f"ğŸ“ˆ å·²å¤„ç† {i + 1} ä¸ªæŠ•èµ„è€…")
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºå‰3ä¸ªè®°å½•çš„è¯¦ç»†ä¿¡æ¯
                    if self.debug and i < 3:
                        print(f"âœ… è§£ææŠ•èµ„è€… {i+1}: {investor_data['name']}")
                        print(f"   - æ’å: {investor_data['rank']}")
                        print(f"   - ç±»å‹: {investor_data['ventureType']}")
                        print(f"   - è¯„åˆ†: {investor_data['rating']}")
                        print(f"   - æŠ•èµ„æ€»æ•°: {investor_data['totalInvestments']}")
                        print(f"   - å¹³å‡å…¬å¼€ROI: {investor_data['avgPublicRoi']}")
                        print(f"   - æŠ•èµ„ç»„åˆé¡¹ç›®æ•°: {len(investor_data['portfolioProjects'])}")
                    
                except Exception as e:
                    print(f"âŒ è§£æç¬¬ {i+1} ä¸ªè®°å½•æ—¶å‡ºé”™: {str(e)}")
                    if self.debug:
                        print(f"ğŸ” é”™è¯¯è®°å½•: {item}")
                    continue
            
            print(f"âœ… æˆåŠŸè§£æ {len(investors)} ä¸ªæŠ•èµ„è€…æ•°æ®")
            return investors
            
        except Exception as e:
            print(f"âŒ è§£æAPIå“åº”æ—¶å‡ºé”™: {str(e)}")
            if self.debug:
                import traceback
                print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return investors

    def _determine_tier(self, rating):
        """æ ¹æ®è¯„åˆ†ç¡®å®šæŠ•èµ„è€…ç­‰çº§"""
        if rating is None:
            return 'Unknown'
        elif rating >= 4:
            return 'S'
        elif rating >= 3:
            return 'A'
        elif rating >= 2:
            return 'B'
        elif rating >= 1:
            return 'C'
        else:
            return 'D'

    def _save_page_to_database(self, investors_data, page_num):
        """ä¿å­˜å•é¡µæ•°æ®åˆ°MongoDBæ•°æ®åº“"""
        try:
            if not investors_data:
                print(f"âš ï¸  ç¬¬ {page_num} é¡µæ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return 0
            
            print(f"ğŸ”„ å¼€å§‹è½¬æ¢ç¬¬ {page_num} é¡µçš„ {len(investors_data)} æ¡æ•°æ®...")
            
            # è½¬æ¢ä¸ºInvestorDataå¯¹è±¡
            investor_objects = []
            conversion_errors = 0
            
            for i, item in enumerate(investors_data, 1):
                try:
                    investor_obj = InvestorData(item)
                    mongo_dict = investor_obj.to_mongo_dict()
                    investor_objects.append(mongo_dict)
                    
                    if self.debug and i <= 2:
                        print(f"âœ… ç¬¬{i}ä¸ªæŠ•èµ„è€…æ•°æ®è½¬æ¢æˆåŠŸ: {mongo_dict.get('name', 'N/A')}")
                        print(f"   - ID: {mongo_dict.get('investor_id', 'N/A')}")
                        print(f"   - æ’å: {mongo_dict.get('rank', 'N/A')}")
                        print(f"   - æ€»æŠ•èµ„æ•°: {mongo_dict.get('total_investments', 'N/A')}")
                except Exception as e:
                    conversion_errors += 1
                    print(f"âš ï¸  ç¬¬{i}ä¸ªæ•°æ®è½¬æ¢å¤±è´¥: {e}")
                    if self.debug:
                        print(f"ğŸ” è½¬æ¢å¤±è´¥çš„æ•°æ®: {item}")
                    continue
            
            if conversion_errors > 0:
                print(f"âš ï¸  æ•°æ®è½¬æ¢å®Œæˆ: {len(investor_objects)} æˆåŠŸ, {conversion_errors} å¤±è´¥")
            
            if not investor_objects:
                print(f"âŒ ç¬¬ {page_num} é¡µæ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
                return 0
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = 0
            update_count = 0
            insert_count = 0
            
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(investor_objects)} æ¡æ•°æ®åˆ°æ•°æ®åº“...")
            
            for i, investor_data in enumerate(investor_objects, 1):
                try:
                    # ä½¿ç”¨upsertæ–¹å¼ï¼Œæ ¹æ®investor_idæˆ–nameæ›´æ–°æˆ–æ’å…¥
                    query = {}
                    if investor_data.get('investor_id'):
                        query['investor_id'] = investor_data['investor_id']
                    else:
                        query['name'] = investor_data['name']
                    
                    result = self.investor_manager.collection.update_one(
                        query,
                        {'$set': investor_data},
                        upsert=True
                    )
                    
                    if result.upserted_id:
                        insert_count += 1
                        saved_count += 1
                        if self.debug and i <= 2:
                            print(f"â• æ–°æ’å…¥: {investor_data['name']}")
                    elif result.modified_count > 0:
                        update_count += 1
                        saved_count += 1
                        if self.debug and i <= 2:
                            print(f"ğŸ”„ å·²æ›´æ–°: {investor_data['name']}")
                    elif self.debug and i <= 2:
                        print(f"â– æ— å˜åŒ–: {investor_data['name']}")
                except Exception as e:
                    print(f"âŒ ä¿å­˜æŠ•èµ„è€…æ•°æ®å¤±è´¥ {investor_data.get('name', 'Unknown')}: {e}")
                    if self.debug:
                        print(f"ğŸ” ä¿å­˜å¤±è´¥çš„æ•°æ®: {investor_data}")
            
            print(f"âœ… ç¬¬ {page_num} é¡µæ•°æ®åº“ä¿å­˜å®Œæˆ: {saved_count}/{len(investor_objects)} æ¡è®°å½•")
            print(f"ğŸ“Š ä¿å­˜è¯¦æƒ…: {insert_count} æ–°å¢, {update_count} æ›´æ–°")
            return saved_count
        except Exception as e:
            print(f"âŒ ç¬¬ {page_num} é¡µä¿å­˜æ•°æ®åˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            if self.debug:
                import traceback
                print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return 0
    
    def scrape_all_investors_data(self, max_pages=370, save_csv=True, save_db=True):
        """çˆ¬å–æ‰€æœ‰æŠ•èµ„è€…æ•°æ®çš„ä¸»æ–¹æ³•"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å– DropsTab æŠ•èµ„è€…æ•°æ® (æœ€å¤š {max_pages} é¡µ)")
        print(f"ğŸ’¾ å®æ—¶ä¿å­˜æ¨¡å¼: æ¯é¡µæ•°æ®å°†ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“")
        
        if self.debug:
            print(f"ğŸ“‹ çˆ¬å–é…ç½®:")
            print(f"   - æœ€å¤§é¡µæ•°: {max_pages}")
            print(f"   - ä¿å­˜CSV: {save_csv}")
            print(f"   - å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“: å¯ç”¨")
            print(f"   - è°ƒè¯•æ¨¡å¼: {self.debug}")
        
        start_time = time.time()
        # æ³¨æ„ï¼šç°åœ¨æ•°æ®åœ¨çˆ¬å–è¿‡ç¨‹ä¸­å·²ç»ä¿å­˜åˆ°æ•°æ®åº“äº†
        investors_data = self.scrape_investors_data(max_pages=max_pages)
        scrape_time = time.time() - start_time
        
        print(f"\nğŸ“Š çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(investors_data)} ä¸ªæŠ•èµ„è€…æ•°æ®")
        print(f"ğŸ’¾ æ‰€æœ‰æ•°æ®å·²å®æ—¶ä¿å­˜åˆ°MongoDBæ•°æ®åº“")
        
        if self.debug:
            print(f"â±ï¸  æ€»çˆ¬å–è€—æ—¶: {scrape_time:.2f}ç§’")
            print(f"ğŸ“ˆ å¹³å‡æ¯ä¸ªæŠ•èµ„è€…è€—æ—¶: {scrape_time/len(investors_data):.3f}ç§’" if investors_data else "")
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if save_csv and investors_data:
            print(f"ğŸ’¾ å¼€å§‹ä¿å­˜CSVæ–‡ä»¶...")
            self.save_to_csv(investors_data)
        
        print(f"ğŸ‰ æ‰€æœ‰æ“ä½œå®Œæˆ!")
        
        return investors_data

    def _parse_investors_page(self, soup):
        """è§£ææŠ•èµ„è€…é¡µé¢æ•°æ®"""
        investors = []
        
        try:
            # æŸ¥æ‰¾æŠ•èµ„è€…è¡¨æ ¼
            table = soup.find('table', class_='table table-striped table-hover')
            if not table:
                print("âŒ æœªæ‰¾åˆ°æŠ•èµ„è€…è¡¨æ ¼")
                return investors
            
            tbody = table.find('tbody')
            if not tbody:
                print("âŒ æœªæ‰¾åˆ°è¡¨æ ¼ä¸»ä½“")
                return investors
            
            rows = tbody.find_all('tr')
            print(f"ğŸ“Š æ‰¾åˆ° {len(rows)} è¡ŒæŠ•èµ„è€…æ•°æ®")
            
            for i, row in enumerate(rows):
                try:
                    cells = row.find_all('td')
                    if len(cells) < 6:
                        print(f"âš ï¸ ç¬¬ {i+1} è¡Œæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                        continue
                    
                    # è§£ææŠ•èµ„è€…åç§°
                    name_cell = cells[0]
                    name_link = name_cell.find('a')
                    name = name_link.text.strip() if name_link else name_cell.text.strip()
                    
                    # è§£ææŠ•èµ„è€…ç±»å‹
                    type_cell = cells[1]
                    investor_type = type_cell.text.strip()
                    
                    # è§£ææˆåŠŸç‡
                    success_rate_cell = cells[2]
                    success_rate_text = success_rate_cell.text.strip()
                    success_rate = None
                    if success_rate_text and success_rate_text != '-':
                        try:
                            success_rate = float(success_rate_text.replace('%', ''))
                        except ValueError:
                            print(f"âš ï¸ æ— æ³•è§£ææˆåŠŸç‡: {success_rate_text}")
                    
                    # è§£æå€æ•°ä¿¡æ¯
                    multiplier_cell = cells[3]
                    multiplier_text = multiplier_cell.text.strip()
                    avg_multiplier = None
                    if multiplier_text and multiplier_text != '-':
                        try:
                            avg_multiplier = float(multiplier_text.replace('x', ''))
                        except ValueError:
                            print(f"âš ï¸ æ— æ³•è§£æå¹³å‡å€æ•°: {multiplier_text}")
                    
                    # è§£æç­‰çº§
                    tier_cell = cells[4]
                    tier = tier_cell.text.strip()
                    
                    # è§£ææœ€åæ´»åŠ¨æ—¶é—´
                    last_activity_cell = cells[5]
                    last_activity = last_activity_cell.text.strip()
                    
                    investor_data = {
                        'name': name,
                        'type': investor_type,
                        'success_rate': success_rate,
                        'avg_multiplier': avg_multiplier,
                        'tier': tier,
                        'last_activity': last_activity,
                        'scraped_at': datetime.utcnow()
                    }
                    
                    investors.append(investor_data)
                    
                    if (i + 1) % 10 == 0:
                        print(f"ğŸ“ˆ å·²å¤„ç† {i + 1} ä¸ªæŠ•èµ„è€…")
                    
                except Exception as e:
                    print(f"âŒ è§£æç¬¬ {i+1} è¡Œæ—¶å‡ºé”™: {str(e)}")
                    continue
            
            print(f"âœ… æˆåŠŸè§£æ {len(investors)} ä¸ªæŠ•èµ„è€…æ•°æ®")
            return investors
            
        except Exception as e:
            print(f"âŒ è§£ææŠ•èµ„è€…é¡µé¢æ—¶å‡ºé”™: {str(e)}")
            
# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = DropstabScraper(page_delay_min=10, page_delay_max=20.0)
    
    # æµ‹è¯•çˆ¬å–å‰å‡ é¡µ
    print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šçˆ¬å–å‰3é¡µæ•°æ®")
    test_data = scraper.scrape_all_investors_data(max_pages=3, save_csv=True, save_db=True)
    
    # æ˜¾ç¤ºå‰5ä¸ªæŠ•èµ„è€…ä¿¡æ¯
    if test_data:
        print("\nğŸ“‹ å‰5ä¸ªæŠ•èµ„è€…ä¿¡æ¯:")
        for i, investor in enumerate(test_data[:5], 1):
            print(f"{i}. {investor.get('æŠ•èµ„è€…åç§°', 'N/A')} - {investor.get('ç±»å‹', 'N/A')} - æˆåŠŸç‡: {investor.get('æˆåŠŸç‡', 'N/A')}")
    
    # å¦‚æœéœ€è¦çˆ¬å–å…¨éƒ¨æ•°æ®ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # print("\nğŸš€ å¼€å§‹çˆ¬å–å…¨éƒ¨æ•°æ®...")
    # full_data = scraper.scrape_all_investors_data(max_pages=370, save_csv=True, save_db=True)


    