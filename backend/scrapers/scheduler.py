#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«è°ƒåº¦å™¨
"""

import time
import random
from flask import current_app
from ..app import scheduler, socketio
from ..database.db import CryptoDataManager
from ..models.crypto import CryptoData
from .coingecko import CoinGeckoScraper
from datetime import datetime, timedelta

# å…¨å±€åº”ç”¨å®ä¾‹
_app_instance = None
_app_config = None  # æ·»åŠ é…ç½®ç¼“å­˜

# æ–°å¢ï¼šç»Ÿä¸€çš„åœæ­¢æ ‡å¿—å­˜å‚¨
_scraper_stop_flags = {
    "coingecko": False,
    "dropstab": False,
    "tokenomist": False,
}


def set_scraper_stop_flag(scraper_type, flag=True):
    """è®¾ç½®çˆ¬è™«åœæ­¢æ ‡å¿—ï¼ˆä¾› /scraper/<type>/stop ä½¿ç”¨ï¼‰"""
    try:
        _scraper_stop_flags[scraper_type] = bool(flag)
        log_and_emit(f"ğŸ›‘ å·²è®¾ç½® {scraper_type} çˆ¬è™«åœæ­¢æ ‡å¿—ä¸º {flag}", "warning")
    except Exception as e:
        print(f"è®¾ç½®åœæ­¢æ ‡å¿—å¤±è´¥: {e}")


def send_log_to_frontend(message, log_type="info"):
    """å‘é€æ—¥å¿—æ¶ˆæ¯åˆ°å‰ç«¯"""
    try:
        socketio.emit(
            "scraper_log",
            {
                "message": message,
                "type": log_type,
                "timestamp": datetime.now().isoformat(),
            },
        )
    except Exception as e:
        print(f"å‘é€æ—¥å¿—åˆ°å‰ç«¯å¤±è´¥: {e}")


def log_and_emit(message, log_type="info"):
    """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œå‘é€åˆ°å‰ç«¯"""
    print(message)
    send_log_to_frontend(message, log_type)


def get_next_random_interval(min_interval, max_interval):
    """è·å–ä¸‹æ¬¡çˆ¬å–çš„éšæœºé—´éš”ï¼ˆç§’ï¼‰"""
    return random.randint(min_interval, max_interval)


def schedule_next_scrape():
    """è°ƒåº¦ä¸‹æ¬¡çˆ¬å–ä»»åŠ¡"""
    try:
        if not _app_config:
            print("âŒ åº”ç”¨é…ç½®æœªåˆå§‹åŒ–")
            return

        min_interval = _app_config.get("SCRAPE_INTERVAL_MIN", 600)
        max_interval = _app_config.get("SCRAPE_INTERVAL_MAX", 1800)

        next_interval = get_next_random_interval(min_interval, max_interval)
        next_run_time = datetime.now() + timedelta(seconds=next_interval)

        # ç§»é™¤ç°æœ‰çš„è°ƒåº¦ä»»åŠ¡
        try:
            scheduler.remove_job("crypto_scraper_scheduled")
        except:
            pass

        # æ·»åŠ æ–°çš„è°ƒåº¦ä»»åŠ¡
        scheduler.add_job(
            func=scrape_crypto_data_and_reschedule_scheduled,
            trigger="date",
            run_date=next_run_time,
            id="crypto_scraper_scheduled",
            name="å®šæ—¶å•é¡µåŠ å¯†è´§å¸æ•°æ®çˆ¬å–",
            replace_existing=True,
        )

        # åŒæ—¶å‘é€åˆ°æ§åˆ¶å°å’Œå‰ç«¯æ—¥å¿—ç³»ç»Ÿ
        next_time_message = f"â° ä¸‹æ¬¡å•é¡µçˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} (é—´éš”: {next_interval // 60}åˆ†{next_interval % 60}ç§’)"
        log_and_emit(next_time_message, "info")

    except Exception as e:
        log_and_emit(f"âŒ è°ƒåº¦ä¸‹æ¬¡çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}", "error")


def scrape_crypto_data_and_reschedule():
    """çˆ¬å–æ•°æ®å¹¶é‡æ–°è°ƒåº¦"""
    scrape_crypto_data()
    schedule_next_scrape()


def scrape_crypto_data():
    """çˆ¬å–åŠ å¯†è´§å¸æ•°æ®çš„å®šæ—¶ä»»åŠ¡ï¼ˆç»Ÿä¸€ä¸ºå•é¡µçˆ¬å–ï¼‰"""
    start_time = datetime.now()
    log_and_emit(
        f"=== å¼€å§‹çˆ¬å–ä»»åŠ¡ {start_time.strftime('%Y-%m-%d %H:%M:%S')} ===", "info"
    )

    try:
        # ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„åº”ç”¨å®ä¾‹ï¼Œä¸ä½¿ç”¨ä»£ç†
        if not _app_instance:
            log_and_emit("âŒ åº”ç”¨å®ä¾‹æœªè®¾ç½®", "error")
            return

        # ä½¿ç”¨æ¨é€çš„åº”ç”¨ä¸Šä¸‹æ–‡
        with _app_instance.app_context():
            log_and_emit("ğŸ“± åº”ç”¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ", "success")

            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            crypto_manager = CryptoDataManager()
            if not crypto_manager.test_connection():
                log_and_emit("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç»ˆæ­¢çˆ¬å–ä»»åŠ¡", "error")
                return

            # è·å–é…ç½® - å¼ºåˆ¶è®¾ç½®ä¸ºå•é¡µçˆ¬å–
            per_page = _app_config.get("CRYPTO_PER_PAGE", 250) if _app_config else 250
            max_pages = 1  # å¼ºåˆ¶è®¾ç½®ä¸º1é¡µ
            page_delay_min = (
                _app_config.get("PAGE_DELAY_MIN", 2.0) if _app_config else 2.0
            )
            page_delay_max = (
                _app_config.get("PAGE_DELAY_MAX", 5.0) if _app_config else 5.0
            )

            log_and_emit(f"ğŸ“Š å¼€å§‹å•é¡µçˆ¬å– - æ¯é¡µ{per_page}ä¸ªå¸ç§", "info")

            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            scraper = CoinGeckoScraper(page_delay_min, page_delay_max)

            # çˆ¬å–å•é¡µæ•°æ® - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            log_and_emit(f"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬ 1/1 é¡µ...", "info")

            # ä½¿ç”¨ scrape_all_crypto_data æ–¹æ³•ï¼Œä½†é™åˆ¶ä¸º1é¡µ
            scraped_data = scraper.scrape_all_crypto_data(
                per_page=per_page, max_pages=max_pages
            )

            if scraped_data:
                log_and_emit(f"âœ… ç¬¬1é¡µçˆ¬å–æˆåŠŸ: {len(scraped_data)}æ¡æ•°æ®", "success")

                # è½¬æ¢ä¸º CryptoData å¯¹è±¡
                crypto_objects = []
                for item in scraped_data:
                    try:
                        crypto_obj = CryptoData(
                            {
                                "id": item.get("id"),
                                "symbol": item.get("symbol"),
                                "name": item.get("name"),
                                "price_usd": item.get("current_price"),
                                "price_change_percentage_24h": item.get(
                                    "price_change_percentage_24h"
                                ),
                                "market_cap": item.get("market_cap"),
                                "volume_24h": item.get("total_volume"),
                                "circulating_supply": item.get("circulating_supply"),
                                "rank": item.get("market_cap_rank"),
                                "source": "coingecko",
                                "timestamp": start_time,  # ä½¿ç”¨ datetime å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
                            }
                        )
                        crypto_objects.append(crypto_obj)
                    except Exception as e:
                        log_and_emit(
                            f"âŒ æ•°æ®è½¬æ¢å¤±è´¥ {item.get('symbol', 'Unknown')}: {e}",
                            "warning",
                        )

                # ä¿å­˜æ•°æ®
                if crypto_objects:
                    saved_count = _save_scraped_data(crypto_objects, start_time)
                    log_and_emit(f"ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ: {saved_count}æ¡", "success")
                else:
                    log_and_emit("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜", "error")
            else:
                log_and_emit(f"âŒ ç¬¬1é¡µçˆ¬å–å¤±è´¥", "error")

            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            log_and_emit(
                f"=== çˆ¬å–ä»»åŠ¡å®Œæˆ {end_time.strftime('%Y-%m-%d %H:%M:%S')} ===", "info"
            )
            log_and_emit(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f}ç§’", "info")

    except Exception as e:
        log_and_emit(f"âŒ çˆ¬å–æ•°æ®æ—¶å‡ºé”™: {e}", "error")
        import traceback

        traceback.print_exc()


def _save_scraped_data(scraped_data, start_time):
    """ä¿å­˜çˆ¬å–çš„æ•°æ®åˆ°æ•°æ®åº“"""
    if not scraped_data:
        return 0

    try:
        crypto_manager = CryptoDataManager()
        saved_count = 0
        duplicate_count = 0

        # ç»Ÿè®¡é‡å¤çš„symbol
        symbol_counts = {}
        for crypto in scraped_data:
            symbol = crypto.symbol
            symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1

        # æŠ¥å‘Šé‡å¤æƒ…å†µ
        duplicates = {
            symbol: count for symbol, count in symbol_counts.items() if count > 1
        }
        if duplicates:
            print(f"âš ï¸  å‘ç°é‡å¤symbol: {duplicates}")
            duplicate_count = sum(count - 1 for count in duplicates.values())

        print(f"ğŸ“Š æ•°æ®ä¿å­˜ç»Ÿè®¡:")
        print(f"  - APIè¿”å›æ•°æ®: {len(scraped_data)}æ¡")
        print(f"  - é‡å¤æ•°æ®: {duplicate_count}æ¡")
        print(f"  - é¢„æœŸä¿å­˜: {len(scraped_data) - duplicate_count}æ¡")

        # åœ¨_save_scraped_dataæ–¹æ³•ä¸­ï¼Œä¸ºæ¯æ¡è®°å½•è®¾ç½®å½“å‰æ—¶é—´æˆ³
        for crypto in scraped_data:
            try:
                # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºtimestampï¼Œç¡®ä¿æ•°æ®çš„æ—¶æ•ˆæ€§
                crypto.timestamp = datetime.now()
                mongo_data = crypto.to_mongo_dict()

                result = crypto_manager.collection.update_one(
                    {"id": crypto.id}, {"$set": mongo_data}, upsert=True
                )

                if result.upserted_id or result.modified_count > 0:
                    saved_count += 1

            except Exception as e:
                print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥ {crypto.symbol}: {e}")

        print(f"âœ… å®é™…ä¿å­˜: {saved_count}æ¡æ•°æ®")

        # éªŒè¯æ•°æ®åº“ä¸­çš„è®°å½•æ•°
        total_in_db = crypto_manager.collection.count_documents({})
        print(f"ğŸ“Š æ•°æ®åº“æ€»è®°å½•æ•°: {total_in_db}æ¡")

        return saved_count

    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
        return 0


def scrape_crypto_data_and_reschedule_scheduled():
    """å®šæ—¶è°ƒåº¦çš„çˆ¬å–ä»»åŠ¡"""
    scrape_crypto_data()
    schedule_next_scrape()


def set_app_instance(app):
    """è®¾ç½®å…¨å±€åº”ç”¨å®ä¾‹"""
    global _app_instance, _app_config
    # å­˜å‚¨å®é™…çš„åº”ç”¨å®ä¾‹ï¼Œè€Œä¸æ˜¯ä»£ç†
    _app_instance = (
        app._get_current_object() if hasattr(app, "_get_current_object") else app
    )
    # ç¼“å­˜é…ç½®
    _app_config = dict(app.config)
    print(f"âœ… åº”ç”¨å®ä¾‹å·²è®¾ç½®: {type(_app_instance)}")


def start_crypto_scraping_jobs(app):
    """å¯åŠ¨åŠ å¯†è´§å¸çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    global _app_instance, _app_config

    set_app_instance(app)

    with app.app_context():
        min_interval = app.config.get("SCRAPE_INTERVAL_MIN", 600)
        max_interval = app.config.get("SCRAPE_INTERVAL_MAX", 1800)

        log_and_emit(
            f"ğŸª™ CoinGecko çˆ¬è™«è°ƒåº¦å·²å¯åŠ¨ (é—´éš”: {min_interval // 60}-{max_interval // 60}åˆ†é’Ÿ)",
            "info",
        )

        # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡çˆ¬å–
        scheduler.add_job(
            func=scrape_crypto_data_and_reschedule,
            trigger="date",
            run_date=datetime.now(),
            id="crypto_scraper_initial",
            name="åˆå§‹åŠ å¯†è´§å¸æ•°æ®çˆ¬å–",
            replace_existing=True,
        )

        schedule_next_scrape()


def start_investor_scraping_jobs(app):
    """å¯åŠ¨æŠ•èµ„è€…çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    global _app_instance, _app_config

    set_app_instance(app)

    with app.app_context():
        min_interval = app.config.get("INVESTOR_SCRAPE_INTERVAL_MIN", 60)  # 1åˆ†é’Ÿ
        max_interval = app.config.get("INVESTOR_SCRAPE_INTERVAL_MAX", 600)  # 10åˆ†é’Ÿ

        log_and_emit(
            f"ğŸ’¼ DropsTab æŠ•èµ„è€…çˆ¬è™«è°ƒåº¦å·²å¯åŠ¨ (é—´éš”: {min_interval // 60}-{max_interval // 60}åˆ†é’Ÿ)",
            "info",
        )

        # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡çˆ¬å–
        scheduler.add_job(
            func=scrape_investor_data_and_reschedule,
            trigger="date",
            run_date=datetime.now(),
            id="investor_scraper_initial",
            name="åˆå§‹æŠ•èµ„è€…æ•°æ®çˆ¬å–",
            replace_existing=True,
        )

        schedule_next_investor_scrape()


def start_tokenomist_scraping_jobs(app):
    """å¯åŠ¨ Tokenomist ä»£å¸è§£é”çˆ¬è™«å®šæ—¶ä»»åŠ¡ï¼ˆå³æ—¶æ‰§è¡Œä¸€æ¬¡å¹¶è°ƒåº¦ä¸‹ä¸€æ¬¡ï¼‰"""
    global _app_instance, _app_config
    set_app_instance(app)
    # æ¸…é™¤åœæ­¢æ ‡å¿—
    set_scraper_stop_flag("tokenomist", False)
    with app.app_context():
        # ä½¿ç”¨ Tokenomist ä¸“ç”¨é…ç½®
        min_interval = app.config.get("TOKENOMIST_SCRAPE_INTERVAL_MIN", 7200)  # 2å°æ—¶
        max_interval = app.config.get("TOKENOMIST_SCRAPE_INTERVAL_MAX", 7200)  # 2å°æ—¶
        log_and_emit(
            f"ğŸ”“ Tokenomist çˆ¬è™«è°ƒåº¦å·²å¯åŠ¨ (é—´éš”: {min_interval // 3600}å°æ—¶)",
            "info",
        )
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        scheduler.add_job(
            func=scrape_tokenomist_data_and_reschedule,
            trigger="date",
            run_date=datetime.now(),
            id="tokenomist_scraper_initial",
            name="åˆå§‹ Tokenomist ä»£å¸è§£é”çˆ¬å–",
            replace_existing=True,
        )
        schedule_next_tokenomist_scrape()


def scrape_tokenomist_data_and_reschedule():
    """æ‰§è¡Œä¸€æ¬¡ Tokenomist çˆ¬å–å¹¶è°ƒåº¦ä¸‹ä¸€æ¬¡"""
    scrape_tokenomist_data()
    schedule_next_tokenomist_scrape()


def scrape_tokenomist_data():
    """Tokenomist ä»£å¸è§£é”çˆ¬å–ä»»åŠ¡ï¼ˆè°ƒç”¨ Playwright è„šæœ¬ï¼‰"""
    start_time = datetime.now()
    log_and_emit(
        f"=== å¼€å§‹ Tokenomist ä»£å¸è§£é”çˆ¬å– {start_time.strftime('%Y-%m-%d %H:%M:%S')} ===",
        "info",
    )
    try:
        if not _app_instance:
            log_and_emit("âŒ åº”ç”¨å®ä¾‹æœªè®¾ç½®", "error")
            return

        # å¦‚æœå·²ä¸‹è¾¾åœæ­¢æ ‡å¿—ï¼Œç›´æ¥è·³è¿‡æœ¬æ¬¡æ‰§è¡Œ
        if _scraper_stop_flags.get("tokenomist"):
            log_and_emit("â¹ï¸ å·²è®¾ç½® Tokenomist åœæ­¢æ ‡å¿—ï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ", "warning")
            return

        with _app_instance.app_context():
            # æ‡’åŠ è½½ï¼Œé¿å…æ—  Playwright æ—¶æ¨¡å—å¯¼å…¥å¤±è´¥å½±å“å…¶ä»–çˆ¬è™«
            from ..scrapers.tokenomist_scraper import TokenomistScraper
            import asyncio

            # æ‰§è¡Œä¸€æ¬¡å¼‚æ­¥çˆ¬å–
            log_and_emit("ğŸš€ æ­£åœ¨å¯åŠ¨ Playwright æ— å¤´æµè§ˆå™¨...", "info")
            # ä¼ å…¥ should_stop å›è°ƒï¼Œä½¿è¿è¡Œä¸­çš„ä»»åŠ¡ä¹Ÿèƒ½åŠæ—¶é€€å‡º
            asyncio.run(
                TokenomistScraper(
                    should_stop=lambda: _scraper_stop_flags.get("tokenomist", False)
                ).run()
            )
            log_and_emit("âœ… Tokenomist çˆ¬å–ä»»åŠ¡å®Œæˆ", "success")

    except Exception as e:
        log_and_emit(f"âŒ Tokenomist çˆ¬å–å¤±è´¥: {e}", "error")


def schedule_next_tokenomist_scrape():
    """è°ƒåº¦ä¸‹ä¸€æ¬¡ Tokenomist çˆ¬å–ï¼ˆä½¿ç”¨ä¸“ç”¨2å°æ—¶é—´éš”ï¼‰ï¼Œå¦‚å·²ä¸‹è¾¾åœæ­¢æ ‡å¿—åˆ™ä¸å†è°ƒåº¦"""
    try:
        if not _app_config:
            print("âŒ åº”ç”¨é…ç½®æœªåˆå§‹åŒ–")
            return

        # å¦‚è®¾ç½®äº†åœæ­¢æ ‡å¿—åˆ™ä¸è°ƒåº¦
        if _scraper_stop_flags.get("tokenomist"):
            log_and_emit("â¹ï¸ å·²è®¾ç½® Tokenomist åœæ­¢æ ‡å¿—ï¼Œåœæ­¢åç»­è°ƒåº¦", "warning")
            try:
                scheduler.remove_job("tokenomist_scraper_scheduled")
            except Exception:
                pass
            return

        # ä½¿ç”¨ Tokenomist ä¸“ç”¨é…ç½®
        min_interval = _app_config.get("TOKENOMIST_SCRAPE_INTERVAL_MIN", 7200)  # 2å°æ—¶
        max_interval = _app_config.get("TOKENOMIST_SCRAPE_INTERVAL_MAX", 7200)  # 2å°æ—¶
        next_interval = get_next_random_interval(min_interval, max_interval)
        next_run_time = datetime.now() + timedelta(seconds=next_interval)

        # å…ˆç§»é™¤æ—§çš„
        try:
            scheduler.remove_job("tokenomist_scraper_scheduled")
        except Exception:
            pass

        scheduler.add_job(
            func=scrape_tokenomist_data_and_reschedule,
            trigger="date",
            run_date=next_run_time,
            id="tokenomist_scraper_scheduled",
            name="å®šæ—¶ Tokenomist ä»£å¸è§£é”çˆ¬å–",
            replace_existing=True,
        )

        log_and_emit(
            f"â° ä¸‹æ¬¡ Tokenomist çˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} (é—´éš”: {next_interval // 3600}å°æ—¶{(next_interval % 3600) // 60}åˆ†é’Ÿ)",
            "info",
        )

    except Exception as e:
        log_and_emit(f"âŒ è°ƒåº¦ä¸‹ä¸€æ¬¡ Tokenomist çˆ¬å–å¤±è´¥: {e}", "error")


# ä¿æŒåŸæœ‰çš„ç»Ÿä¸€å¯åŠ¨æ–¹æ³•ä»¥å…¼å®¹æ—§æ¥å£
def start_scraping_jobs(app):
    """å¯åŠ¨æ‰€æœ‰çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    start_crypto_scraping_jobs(app)
    start_investor_scraping_jobs(app)
    # å¯é€‰ï¼šåŒæ—¶å¯åŠ¨ Tokenomistï¼ˆå¦‚ä¸å¸Œæœ›é»˜è®¤å¯åŠ¨å¯æ³¨é‡Šæ‰ï¼‰
    # start_tokenomist_scraping_jobs(app)


def scrape_investor_data_and_reschedule():
    """çˆ¬å–æŠ•èµ„è€…æ•°æ®å¹¶é‡æ–°è°ƒåº¦"""
    scrape_investor_data()
    schedule_next_investor_scrape()


def scrape_investor_data():
    """æ‰§è¡Œä¸€æ¬¡ DropsTab æŠ•èµ„è€…æ•°æ®çˆ¬å–"""
    start_time = datetime.now()
    log_and_emit(
        f"=== å¼€å§‹ DropsTab æŠ•èµ„è€…æ•°æ®çˆ¬å– {start_time.strftime('%Y-%m-%d %H:%M:%S')} ===",
        "info",
    )
    try:
        if not _app_instance:
            log_and_emit("âŒ åº”ç”¨å®ä¾‹æœªè®¾ç½®", "error")
            return

        with _app_instance.app_context():
            # æ‡’åŠ è½½ï¼Œé¿å…å¯¼å…¥å¼€é”€æˆ–ä¾èµ–é—®é¢˜
            from ..scrapers.dropstab import DropstabScraper

            # è¯»å–å¯é€‰çš„æœ€å¤§é¡µæ•°é…ç½®ï¼ˆå¦‚æœªé…ç½®åˆ™ä½¿ç”¨çˆ¬è™«é»˜è®¤çš„ 370ï¼‰
            max_pages = None
            if _app_config:
                max_pages = _app_config.get("INVESTOR_MAX_PAGES", None)

            scraper = DropstabScraper()
            if max_pages is not None:
                log_and_emit(f"ğŸš€ æ­£åœ¨çˆ¬å–æŠ•èµ„è€…æ•°æ®ï¼ˆæœ€å¤š {max_pages} é¡µï¼‰...", "info")
                scraper.scrape_investors_data(max_pages=max_pages)
            else:
                log_and_emit("ğŸš€ æ­£åœ¨çˆ¬å–æŠ•èµ„è€…æ•°æ®ï¼ˆä½¿ç”¨é»˜è®¤æœ€å¤§é¡µæ•°ï¼‰...", "info")
                scraper.scrape_investors_data()

            log_and_emit("âœ… æŠ•èµ„è€…æ•°æ®çˆ¬å–å®Œæˆ", "success")

    except Exception as e:
        log_and_emit(f"âŒ æŠ•èµ„è€…æ•°æ®çˆ¬å–å¤±è´¥: {e}", "error")


def schedule_next_investor_scrape():
    """è°ƒåº¦ä¸‹æ¬¡ DropsTab æŠ•èµ„è€…æ•°æ®çˆ¬å–"""
    try:
        if not _app_config:
            print("âŒ åº”ç”¨é…ç½®æœªåˆå§‹åŒ–")
            return

        # å¦‚æœè®¾ç½®äº†åœæ­¢æ ‡å¿—åˆ™ä¸å†è°ƒåº¦
        if _scraper_stop_flags.get("dropstab"):
            log_and_emit("â¹ï¸ å·²è®¾ç½® DropsTab åœæ­¢æ ‡å¿—ï¼Œåœæ­¢åç»­è°ƒåº¦", "warning")
            try:
                scheduler.remove_job("investor_scraper_scheduled")
            except Exception:
                pass
            return

        min_interval = _app_config.get("INVESTOR_SCRAPE_INTERVAL_MIN", 60)
        max_interval = _app_config.get("INVESTOR_SCRAPE_INTERVAL_MAX", 600)
        next_interval = get_next_random_interval(min_interval, max_interval)
        next_run_time = datetime.now() + timedelta(seconds=next_interval)

        # å…ˆç§»é™¤æ—§ä»»åŠ¡ï¼ˆè‹¥å­˜åœ¨ï¼‰
        try:
            scheduler.remove_job("investor_scraper_scheduled")
        except Exception:
            pass

        scheduler.add_job(
            func=scrape_investor_data_and_reschedule,
            trigger="date",
            run_date=next_run_time,
            id="investor_scraper_scheduled",
            name="å®šæ—¶æŠ•èµ„è€…æ•°æ®çˆ¬å–",
            replace_existing=True,
        )

        log_and_emit(
            f"â° ä¸‹æ¬¡æŠ•èµ„è€…æ•°æ®çˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} (é—´éš”: {next_interval // 60}åˆ†{next_interval % 60}ç§’)",
            "info",
        )
    except Exception as e:
        log_and_emit(f"âŒ è°ƒåº¦ä¸‹ä¸€æ¬¡æŠ•èµ„è€…æ•°æ®çˆ¬å–å¤±è´¥: {e}", "error")
