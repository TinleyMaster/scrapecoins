#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«è°ƒåº¦å™¨
"""

import time
import random
from flask import current_app
from ..app import scheduler, socketio
from ..database.db import CryptoDataManager
from ..models.crypto import CryptoData
from .coingecko import CoinGeckoScraper
from datetime import datetime, timedelta

# å…¨å±€åº”ç”¨å®ä¾‹
_app_instance = None
_app_config = None  # æ·»åŠ é…ç½®ç¼“å­˜

def send_log_to_frontend(message, log_type='info'):
    """å‘é€æ—¥å¿—æ¶ˆæ¯åˆ°å‰ç«¯"""
    try:
        socketio.emit('scraper_log', {
            'message': message,
            'type': log_type,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        print(f"å‘é€æ—¥å¿—åˆ°å‰ç«¯å¤±è´¥: {e}")

def log_and_emit(message, log_type='info'):
    """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œå‘é€åˆ°å‰ç«¯"""
    print(message)
    send_log_to_frontend(message, log_type)

def get_next_random_interval(min_interval, max_interval):
    """è·å–ä¸‹æ¬¡çˆ¬å–çš„éšæœºé—´éš”ï¼ˆç§’ï¼‰"""
    return random.randint(min_interval, max_interval)

def schedule_next_scrape():
    """è°ƒåº¦ä¸‹æ¬¡çˆ¬å–ä»»åŠ¡"""
    try:
        if not _app_config:
            print("âŒ åº”ç”¨é…ç½®æœªåˆå§‹åŒ–")
            return
            
        min_interval = _app_config.get('SCRAPE_INTERVAL_MIN', 600)
        max_interval = _app_config.get('SCRAPE_INTERVAL_MAX', 1800)
        
        next_interval = get_next_random_interval(min_interval, max_interval)
        next_run_time = datetime.now() + timedelta(seconds=next_interval)
        
        # ç§»é™¤ç°æœ‰çš„è°ƒåº¦ä»»åŠ¡
        try:
            scheduler.remove_job('crypto_scraper_scheduled')
        except:
            pass
        
        # æ·»åŠ æ–°çš„è°ƒåº¦ä»»åŠ¡
        scheduler.add_job(
            func=scrape_crypto_data_and_reschedule_scheduled,
            trigger="date",
            run_date=next_run_time,
            id='crypto_scraper_scheduled',
            name='å®šæ—¶å•é¡µåŠ å¯†è´§å¸æ•°æ®çˆ¬å–',
            replace_existing=True
        )
        
        # åŒæ—¶å‘é€åˆ°æ§åˆ¶å°å’Œå‰ç«¯æ—¥å¿—ç³»ç»Ÿ
        next_time_message = f"â° ä¸‹æ¬¡å•é¡µçˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} (é—´éš”: {next_interval//60}åˆ†{next_interval%60}ç§’)"
        log_and_emit(next_time_message, 'info')
        
    except Exception as e:
        log_and_emit(f"âŒ è°ƒåº¦ä¸‹æ¬¡çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}", 'error')

def scrape_crypto_data_and_reschedule():
    """çˆ¬å–æ•°æ®å¹¶é‡æ–°è°ƒåº¦"""
    scrape_crypto_data()
    schedule_next_scrape()

def scrape_crypto_data():
    """çˆ¬å–åŠ å¯†è´§å¸æ•°æ®çš„å®šæ—¶ä»»åŠ¡ï¼ˆç»Ÿä¸€ä¸ºå•é¡µçˆ¬å–ï¼‰"""
    start_time = datetime.now()
    log_and_emit(f"=== å¼€å§‹çˆ¬å–ä»»åŠ¡ {start_time.strftime('%Y-%m-%d %H:%M:%S')} ===", 'info')
    
    try:
        # ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„åº”ç”¨å®ä¾‹ï¼Œä¸ä½¿ç”¨ä»£ç†
        if not _app_instance:
            log_and_emit("âŒ åº”ç”¨å®ä¾‹æœªè®¾ç½®", 'error')
            return
            
        # ä½¿ç”¨æ¨é€çš„åº”ç”¨ä¸Šä¸‹æ–‡
        with _app_instance.app_context():
            log_and_emit("ğŸ“± åº”ç”¨ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ", 'success')
            
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            crypto_manager = CryptoDataManager()
            if not crypto_manager.test_connection():
                log_and_emit("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç»ˆæ­¢çˆ¬å–ä»»åŠ¡", 'error')
                return
            
            # è·å–é…ç½® - å¼ºåˆ¶è®¾ç½®ä¸ºå•é¡µçˆ¬å–
            per_page = _app_config.get('CRYPTO_PER_PAGE', 250) if _app_config else 250
            max_pages = 1  # å¼ºåˆ¶è®¾ç½®ä¸º1é¡µ
            page_delay_min = _app_config.get('PAGE_DELAY_MIN', 2.0) if _app_config else 2.0
            page_delay_max = _app_config.get('PAGE_DELAY_MAX', 5.0) if _app_config else 5.0
            
            log_and_emit(f"ğŸ“Š å¼€å§‹å•é¡µçˆ¬å– - æ¯é¡µ{per_page}ä¸ªå¸ç§", 'info')
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            scraper = CoinGeckoScraper(page_delay_min, page_delay_max)
            
            # çˆ¬å–å•é¡µæ•°æ® - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            log_and_emit(f"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬ 1/1 é¡µ...", 'info')
            
            # ä½¿ç”¨ scrape_all_crypto_data æ–¹æ³•ï¼Œä½†é™åˆ¶ä¸º1é¡µ
            scraped_data = scraper.scrape_all_crypto_data(per_page=per_page, max_pages=max_pages)
            
            if scraped_data:
                log_and_emit(f"âœ… ç¬¬1é¡µçˆ¬å–æˆåŠŸ: {len(scraped_data)}æ¡æ•°æ®", 'success')
                
                # è½¬æ¢ä¸º CryptoData å¯¹è±¡
                crypto_objects = []
                for item in scraped_data:
                    try:
                        crypto_obj = CryptoData({
                            'id': item.get('id'),
                            'symbol': item.get('symbol'),
                            'name': item.get('name'),
                            'price_usd': item.get('current_price'),
                            'price_change_percentage_24h': item.get('price_change_percentage_24h'),
                            'market_cap': item.get('market_cap'),
                            'volume_24h': item.get('total_volume'),
                            'circulating_supply': item.get('circulating_supply'),
                            'rank': item.get('market_cap_rank'),
                            'source': 'coingecko',
                            'timestamp': start_time  # ä½¿ç”¨ datetime å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²
                        })
                        crypto_objects.append(crypto_obj)
                    except Exception as e:
                        log_and_emit(f"âŒ æ•°æ®è½¬æ¢å¤±è´¥ {item.get('symbol', 'Unknown')}: {e}", 'warning')
                
                # ä¿å­˜æ•°æ®
                if crypto_objects:
                    saved_count = _save_scraped_data(crypto_objects, start_time)
                    log_and_emit(f"ğŸ’¾ æ•°æ®ä¿å­˜å®Œæˆ: {saved_count}æ¡", 'success')
                else:
                    log_and_emit("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜", 'error')
            else:
                log_and_emit(f"âŒ ç¬¬1é¡µçˆ¬å–å¤±è´¥", 'error')
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            log_and_emit(f"=== çˆ¬å–ä»»åŠ¡å®Œæˆ {end_time.strftime('%Y-%m-%d %H:%M:%S')} ===", 'info')
            log_and_emit(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f}ç§’", 'info')
            
    except Exception as e:
        log_and_emit(f"âŒ çˆ¬å–æ•°æ®æ—¶å‡ºé”™: {e}", 'error')
        import traceback
        traceback.print_exc()

def _save_scraped_data(scraped_data, start_time):
    """ä¿å­˜çˆ¬å–çš„æ•°æ®åˆ°æ•°æ®åº“"""
    if not scraped_data:
        return 0
    
    try:
        crypto_manager = CryptoDataManager()
        saved_count = 0
        duplicate_count = 0
        
        # ç»Ÿè®¡é‡å¤çš„symbol
        symbol_counts = {}
        for crypto in scraped_data:
            symbol = crypto.symbol
            symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1
        
        # æŠ¥å‘Šé‡å¤æƒ…å†µ
        duplicates = {symbol: count for symbol, count in symbol_counts.items() if count > 1}
        if duplicates:
            print(f"âš ï¸  å‘ç°é‡å¤symbol: {duplicates}")
            duplicate_count = sum(count - 1 for count in duplicates.values())
        
        print(f"ğŸ“Š æ•°æ®ä¿å­˜ç»Ÿè®¡:")
        print(f"  - APIè¿”å›æ•°æ®: {len(scraped_data)}æ¡")
        print(f"  - é‡å¤æ•°æ®: {duplicate_count}æ¡")
        print(f"  - é¢„æœŸä¿å­˜: {len(scraped_data) - duplicate_count}æ¡")
        
        # åœ¨_save_scraped_dataæ–¹æ³•ä¸­ï¼Œä¸ºæ¯æ¡è®°å½•è®¾ç½®å½“å‰æ—¶é—´æˆ³
        for crypto in scraped_data:
            try:
                # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºtimestampï¼Œç¡®ä¿æ•°æ®çš„æ—¶æ•ˆæ€§
                crypto.timestamp = datetime.now()
                mongo_data = crypto.to_mongo_dict()
                
                result = crypto_manager.collection.update_one(
                    {'id': crypto.id},
                    {'$set': mongo_data},
                    upsert=True
                )
                
                if result.upserted_id or result.modified_count > 0:
                    saved_count += 1
                    
            except Exception as e:
                print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥ {crypto.symbol}: {e}")
        
        print(f"âœ… å®é™…ä¿å­˜: {saved_count}æ¡æ•°æ®")
        
        # éªŒè¯æ•°æ®åº“ä¸­çš„è®°å½•æ•°
        total_in_db = crypto_manager.collection.count_documents({})
        print(f"ğŸ“Š æ•°æ®åº“æ€»è®°å½•æ•°: {total_in_db}æ¡")
        
        return saved_count
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
        return 0

def scrape_crypto_data_and_reschedule_scheduled():
    """å®šæ—¶è°ƒåº¦çš„çˆ¬å–ä»»åŠ¡"""
    scrape_crypto_data()
    schedule_next_scrape()

def set_app_instance(app):
    """è®¾ç½®å…¨å±€åº”ç”¨å®ä¾‹"""
    global _app_instance, _app_config
    # å­˜å‚¨å®é™…çš„åº”ç”¨å®ä¾‹ï¼Œè€Œä¸æ˜¯ä»£ç†
    _app_instance = app._get_current_object() if hasattr(app, '_get_current_object') else app
    # ç¼“å­˜é…ç½®
    _app_config = dict(app.config)
    print(f"âœ… åº”ç”¨å®ä¾‹å·²è®¾ç½®: {type(_app_instance)}")

def start_crypto_scraping_jobs(app):
    """å¯åŠ¨åŠ å¯†è´§å¸çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    global _app_instance, _app_config
    
    set_app_instance(app)
    
    with app.app_context():
        min_interval = app.config.get('SCRAPE_INTERVAL_MIN', 600)
        max_interval = app.config.get('SCRAPE_INTERVAL_MAX', 1800)
        
        log_and_emit(f"ğŸª™ CoinGecko çˆ¬è™«è°ƒåº¦å·²å¯åŠ¨ (é—´éš”: {min_interval//60}-{max_interval//60}åˆ†é’Ÿ)", 'info')
        
        # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡çˆ¬å–
        scheduler.add_job(
            func=scrape_crypto_data_and_reschedule,
            trigger="date",
            run_date=datetime.now(),
            id='crypto_scraper_initial',
            name='åˆå§‹åŠ å¯†è´§å¸æ•°æ®çˆ¬å–',
            replace_existing=True
        )
        
        schedule_next_scrape()

def start_investor_scraping_jobs(app):
    """å¯åŠ¨æŠ•èµ„è€…çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    global _app_instance, _app_config
    
    set_app_instance(app)
    
    with app.app_context():
        min_interval = app.config.get('INVESTOR_SCRAPE_INTERVAL_MIN', 60)  # 1åˆ†é’Ÿ
        max_interval = app.config.get('INVESTOR_SCRAPE_INTERVAL_MAX', 600)  # 10åˆ†é’Ÿ
        
        log_and_emit(f"ğŸ’¼ DropsTab æŠ•èµ„è€…çˆ¬è™«è°ƒåº¦å·²å¯åŠ¨ (é—´éš”: {min_interval//60}-{max_interval//60}åˆ†é’Ÿ)", 'info')
        
        # ç«‹å³æ‰§è¡Œç¬¬ä¸€æ¬¡çˆ¬å–
        scheduler.add_job(
            func=scrape_investor_data_and_reschedule,
            trigger="date",
            run_date=datetime.now(),
            id='investor_scraper_initial',
            name='åˆå§‹æŠ•èµ„è€…æ•°æ®çˆ¬å–',
            replace_existing=True
        )
        
        schedule_next_investor_scrape()

def scrape_investor_data_and_reschedule():
    """çˆ¬å–æŠ•èµ„è€…æ•°æ®å¹¶é‡æ–°è°ƒåº¦"""
    scrape_investor_data()
    schedule_next_investor_scrape()

def scrape_investor_data():
    """çˆ¬å–æŠ•èµ„è€…æ•°æ®çš„å®šæ—¶ä»»åŠ¡"""
    start_time = datetime.now()
    log_and_emit(f"=== å¼€å§‹æŠ•èµ„è€…æ•°æ®çˆ¬å–ä»»åŠ¡ {start_time.strftime('%Y-%m-%d %H:%M:%S')} ===", 'info')
    
    try:
        if not _app_instance:
            log_and_emit("âŒ åº”ç”¨å®ä¾‹æœªè®¾ç½®", 'error')
            return
            
        with _app_instance.app_context():
            from ..database.db import InvestorDataManager
            from ..scrapers.dropstab import DropstabScraper
            
            investor_manager = InvestorDataManager()
            if not investor_manager.test_connection():
                log_and_emit("âŒ æŠ•èµ„è€…æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œç»ˆæ­¢çˆ¬å–ä»»åŠ¡", 'error')
                return
            
            log_and_emit("ğŸš€ å¼€å§‹çˆ¬å– DropsTab æŠ•èµ„è€…æ•°æ®...", 'info')
            
            scraper = DropstabScraper()
            scraped_data = scraper.scrape_all_investors_data(save_csv=False, save_db=True)
            
            if scraped_data:
                log_and_emit(f"âœ… æŠ•èµ„è€…æ•°æ®çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(scraped_data)} æ¡æ•°æ®", 'success')
            else:
                log_and_emit("âš ï¸ æŠ•èµ„è€…æ•°æ®çˆ¬å–å®Œæˆï¼Œä½†æœªè·å–åˆ°æ•°æ®", 'warning')
                
    except Exception as e:
        log_and_emit(f"âŒ æŠ•èµ„è€…æ•°æ®çˆ¬å–å¤±è´¥: {e}", 'error')

def schedule_next_investor_scrape():
    """è°ƒåº¦ä¸‹æ¬¡æŠ•èµ„è€…æ•°æ®çˆ¬å–ä»»åŠ¡"""
    try:
        if not _app_config:
            print("âŒ åº”ç”¨é…ç½®æœªåˆå§‹åŒ–")
            return
            
        min_interval = _app_config.get('INVESTOR_SCRAPE_INTERVAL_MIN', 3600)
        max_interval = _app_config.get('INVESTOR_SCRAPE_INTERVAL_MAX', 7200)
        
        next_interval = get_next_random_interval(min_interval, max_interval)
        next_run_time = datetime.now() + timedelta(seconds=next_interval)
        
        # ç§»é™¤ç°æœ‰çš„è°ƒåº¦ä»»åŠ¡
        try:
            scheduler.remove_job('investor_scraper_scheduled')
        except:
            pass
        
        # æ·»åŠ æ–°çš„è°ƒåº¦ä»»åŠ¡
        scheduler.add_job(
            func=scrape_investor_data_and_reschedule,
            trigger="date",
            run_date=next_run_time,
            id='investor_scraper_scheduled',
            name='å®šæ—¶æŠ•èµ„è€…æ•°æ®çˆ¬å–',
            replace_existing=True
        )
        
        next_time_message = f"â° ä¸‹æ¬¡æŠ•èµ„è€…æ•°æ®çˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} (é—´éš”: {next_interval//60}åˆ†{next_interval%60}ç§’)"
        log_and_emit(next_time_message, 'info')
        
    except Exception as e:
        log_and_emit(f"âŒ è°ƒåº¦ä¸‹æ¬¡æŠ•èµ„è€…æ•°æ®çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}", 'error')

# ä¿æŒåŸæœ‰çš„ç»Ÿä¸€å¯åŠ¨æ–¹æ³•ä»¥å…¼å®¹æ—§æ¥å£
def start_scraping_jobs(app):
    """å¯åŠ¨æ‰€æœ‰çˆ¬è™«å®šæ—¶ä»»åŠ¡"""
    start_crypto_scraping_jobs(app)
    start_investor_scraping_jobs(app)
